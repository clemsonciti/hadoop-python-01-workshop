{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "layout: lesson\n",
    "title: Introduction to Hadoop\n",
    "subtitle: Interacting with Hadoop\n",
    "minutes: 20\n",
    "---\n",
    "\n",
    "> ## Learning objectives {.objectives}\n",
    "> * Learn how to use the Hadoop command in Jupyter shells.\n",
    "> * Learn how to access the web UI of the Hadoop Distributed File System.\n",
    "\n",
    "In this workshop, we will leverage the Jupyter infrastructure at Clemson\n",
    "University to directly interact with Hadoop. Technical details on how to use \n",
    "the Jupyter infrastructure can be found at **https://clemsonciti.github.io/jupyterhub-userdocs/**\n",
    "\n",
    "For this workshop, the default codes inside a cell will be interpreted as Python\n",
    " language. However, any line that begins with **!** will be interpreted as a\n",
    " Linux system command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "print (\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin\t    common1  feltus\t    lost+found\tpanicle   selinux   usr\r\n",
      "bioengr     cugi     home\t    media\tproc\t  smlc\t    var\r\n",
      "boot\t    cvmfs    install\t    misc\troot\t  software  xcatpost\r\n",
      "cgroup\t    dev      lib\t    mnt\t\tsbin\t  srv\t    zfs\r\n",
      "collective  etc      lib64\t    net\t\tscratch1  sys\r\n",
      "common\t    fast     local_scratch  opt\t\tscratch2  tmp\r\n"
     ]
    }
   ],
   "source": [
    "!ls /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS commands\n",
    "HDFS provides a set of commands for users to interact with the system from a\n",
    " Linux-based terminal. To view all available HDFS systems commands, run the\n",
    " following in a cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hdfs [--config confdir] [--loglevel loglevel] COMMAND\r\n",
      "       where COMMAND is one of:\r\n",
      "  dfs                  run a filesystem command on the file systems supported in Hadoop.\r\n",
      "  classpath            prints the classpath\r\n",
      "  namenode -format     format the DFS filesystem\r\n",
      "  secondarynamenode    run the DFS secondary namenode\r\n",
      "  namenode             run the DFS namenode\r\n",
      "  journalnode          run the DFS journalnode\r\n",
      "  zkfc                 run the ZK Failover Controller daemon\r\n",
      "  datanode             run a DFS datanode\r\n",
      "  dfsadmin             run a DFS admin client\r\n",
      "  envvars              display computed Hadoop environment variables\r\n",
      "  haadmin              run a DFS HA admin client\r\n",
      "  fsck                 run a DFS filesystem checking utility\r\n",
      "  balancer             run a cluster balancing utility\r\n",
      "  jmxget               get JMX exported values from NameNode or DataNode.\r\n",
      "  mover                run a utility to move block replicas across\r\n",
      "                       storage types\r\n",
      "  oiv                  apply the offline fsimage viewer to an fsimage\r\n",
      "  oiv_legacy           apply the offline fsimage viewer to an legacy fsimage\r\n",
      "  oev                  apply the offline edits viewer to an edits file\r\n",
      "  fetchdt              fetch a delegation token from the NameNode\r\n",
      "  getconf              get config values from configuration\r\n",
      "  groups               get the groups which users belong to\r\n",
      "  snapshotDiff         diff two snapshots of a directory or diff the\r\n",
      "                       current directory contents with a snapshot\r\n",
      "  lsSnapshottableDir   list all snapshottable dirs owned by the current user\r\n",
      "\t\t\t\t\t\tUse -help to see options\r\n",
      "  portmap              run a portmap service\r\n",
      "  nfs3                 run an NFS version 3 gateway\r\n",
      "  cacheadmin           configure the HDFS cache\r\n",
      "  crypto               configure HDFS encryption zones\r\n",
      "  storagepolicies      list/get/set block storage policies\r\n",
      "  version              print the version\r\n",
      "\r\n",
      "Most commands print help when invoked w/o parameters.\r\n"
     ]
    }
   ],
   "source": [
    "!ssh dsciu001 hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this workshop, we are interested in file system commands. Create a new cell\n",
    " and run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512m; support was removed in 8.0\n",
      "Usage: hadoop fs [generic options]\n",
      "\t[-appendToFile <localsrc> ... <dst>]\n",
      "\t[-cat [-ignoreCrc] <src> ...]\n",
      "\t[-checksum <src> ...]\n",
      "\t[-chgrp [-R] GROUP PATH...]\n",
      "\t[-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]\n",
      "\t[-chown [-R] [OWNER][:[GROUP]] PATH...]\n",
      "\t[-copyFromLocal [-f] [-p] [-l] <localsrc> ... <dst>]\n",
      "\t[-copyToLocal [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
      "\t[-count [-q] [-h] [-v] [-t [<storage type>]] <path> ...]\n",
      "\t[-cp [-f] [-p | -p[topax]] <src> ... <dst>]\n",
      "\t[-createSnapshot <snapshotDir> [<snapshotName>]]\n",
      "\t[-deleteSnapshot <snapshotDir> <snapshotName>]\n",
      "\t[-df [-h] [<path> ...]]\n",
      "\t[-du [-s] [-h] <path> ...]\n",
      "\t[-expunge]\n",
      "\t[-find <path> ... <expression> ...]\n",
      "\t[-get [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
      "\t[-getfacl [-R] <path>]\n",
      "\t[-getfattr [-R] {-n name | -d} [-e en] <path>]\n",
      "\t[-getmerge [-nl] <src> <localdst>]\n",
      "\t[-help [cmd ...]]\n",
      "\t[-ls [-d] [-h] [-R] [<path> ...]]\n",
      "\t[-mkdir [-p] <path> ...]\n",
      "\t[-moveFromLocal <localsrc> ... <dst>]\n",
      "\t[-moveToLocal <src> <localdst>]\n",
      "\t[-mv <src> ... <dst>]\n",
      "\t[-put [-f] [-p] [-l] <localsrc> ... <dst>]\n",
      "\t[-renameSnapshot <snapshotDir> <oldName> <newName>]\n",
      "\t[-rm [-f] [-r|-R] [-skipTrash] [-safely] <src> ...]\n",
      "\t[-rmdir [--ignore-fail-on-non-empty] <dir> ...]\n",
      "\t[-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]\n",
      "\t[-setfattr {-n name [-v value] | -x name} <path>]\n",
      "\t[-setrep [-R] [-w] <rep> <path> ...]\n",
      "\t[-stat [format] <path> ...]\n",
      "\t[-tail [-f] <file>]\n",
      "\t[-test -[defsz] <path>]\n",
      "\t[-text [-ignoreCrc] <src> ...]\n",
      "\t[-touchz <path> ...]\n",
      "\t[-truncate [-w] <length> <path> ...]\n",
      "\t[-usage [cmd ...]]\n",
      "\n",
      "Generic options supported are\n",
      "-conf <configuration file>     specify an application configuration file\n",
      "-D <property=value>            use value for given property\n",
      "-fs <local|namenode:port>      specify a namenode\n",
      "-jt <local|resourcemanager:port>    specify a ResourceManager\n",
      "-files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster\n",
      "-libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.\n",
      "-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.\n",
      "\n",
      "The general command line syntax is\n",
      "bin/hadoop command [genericOptions] [commandOptions]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!ssh dsciu001 hdfs dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that HDFS provides a number of file system commands that are quite\n",
    "similar to their Linux counterpart. For example, ***-chown*** and ***-chmod***\n",
    " change ownership and permission of HDFS files and directories, ***-ls*** lists\n",
    "  content of a directory, ***-mkdir*** creates new directory, ***-rm*** removes\n",
    "   files and directories, and so on.\n",
    "\n",
    "> ## ***hadoop fs*** and ***hdfs dfs*** {.callout}\n",
    ">\n",
    "> ***hadoop fs*** is an older syntax for ***hdfs dfs***. While both commands\n",
    "> produce the same results, you are encouraged to use ***hdfs dfs*** instead.\n",
    "\n",
    "When a Hadoop cluster is first started, there is no data. Users usually import\n",
    " data into the cluster from the traditional Linux-based file system. This is\n",
    " done by using the commandOption ***-put***. Vice versa, to move data from HDFS\n",
    " back to a Linux-based file system, commandOption ***-get*** is used.\n",
    "\n",
    "> ## Hadoop Distributed File System is not the Linux File System {.callout}\n",
    ">\n",
    "> It is important to distinguish between the files and directories that are\n",
    "> stored on HDFS and those that are stored on the Linux File Systems. In the\n",
    "> Hadoop usage guide, the prefix ***local*** implies a path to a file/directory\n",
    "> that is on a Linux File System. Anything else implies a path to a\n",
    "> file/directory on HDFS\n",
    "\n",
    "## Check your understanding: Using Jupyter shell to download data {.callout}\n",
    "\n",
    "Create a directory named **intro-to-hadoop** in your home directory on Palmetto\n",
    "\n",
    "From inside this directory, run the following command to get data from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-07-22 12:29:05--  https://github.com/clemsonciti/hadoop-python-01-workshop/raw/gh-pages/data/gutenberg-shakespeare.txt\n",
      "Resolving github.com... 192.30.253.112\n",
      "Connecting to github.com|192.30.253.112|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/clemsonciti/hadoop-python-01-workshop/gh-pages/data/gutenberg-shakespeare.txt [following]\n",
      "--2016-07-22 12:29:06--  https://raw.githubusercontent.com/clemsonciti/hadoop-python-01-workshop/gh-pages/data/gutenberg-shakespeare.txt\n",
      "Resolving raw.githubusercontent.com... 151.101.20.133\n",
      "Connecting to raw.githubusercontent.com|151.101.20.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5447744 (5.2M) [text/plain]\n",
      "Saving to: “gutenberg-shakespeare.txt”\n",
      "\n",
      "100%[======================================>] 5,447,744   13.7M/s   in 0.4s    \n",
      "\n",
      "2016-07-22 12:29:07 (13.7 MB/s) - “gutenberg-shakespeare.txt” saved [5447744/5447744]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/clemsonciti/hadoop-python-01-workshop/raw/gh-pages/data/gutenberg-shakespeare.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check your understanding: View files and directories on HDFS {.challenge}\n",
    "\n",
    "View the content of your HDFS user directory (/user/**your-username**) on\n",
    " Cypress\n",
    "\n",
    "## Check your understanding: Create directory on HDFS {.challenge}\n",
    "\n",
    "Create a directory in your HDFS user directory named **intro-to-hadoop**\n",
    "\n",
    "## Check your understanding: Import file to HDFS {.challenge}\n",
    "\n",
    "Copy the file ***gutenberg-shakespeare.txt*** from Palmetto to this newly\n",
    " created **intro-to-hadoop** directory on HDFS using **put**. View the content of\n",
    " the **intro-to-hadoop** directory to confirm that the file has been\n",
    " successfully uploaded.\n",
    " \n",
    "Use `wget` to download the movie rating data from https://github.com/clemsonciti/hadoop-python-01-workshop/raw/gh-pages/data/ml-10m.zip. Use `unzip` to decompress the file. Copy the decompressed directory, **ml-10M100K**, into the\n",
    "**intro-to-hadoop** directory on HDFS using **put**. Run `hdfs fsck` on this directory to validate the status of \n",
    "the uploaded directory. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda 2.5.0 (Python 3)",
   "language": "python",
   "name": "anaconda_2.5.0_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
