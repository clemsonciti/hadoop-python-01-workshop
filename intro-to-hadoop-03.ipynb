{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Introduction to Hadoop MapReduce </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First principle of optimizing Hadoop workflow: **Reduce data movement in the shuffle phase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/09/11 15:15:00 INFO fs.TrashPolicyDefault: Moved: 'hdfs://dsci/user/lngo/intro-to-hadoop/output-movielens-02' to trash at: hdfs://dsci/user/lngo/.Trash/Current/user/lngo/intro-to-hadoop/output-movielens-021505157300121\n",
      "17/09/11 15:15:01 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/home/lngo/git/intro-to-hadoop-python/avgRatingMapper04.py, /home/lngo/git/intro-to-hadoop-python/avgRatingReducer01.py, /home/lngo/git/intro-to-hadoop-python/movielens/movies.csv] [/usr/hdp/2.6.0.3-8/hadoop-mapreduce/hadoop-streaming-2.7.3.2.6.0.3-8.jar] /hadoop_java_io_tmpdir/streamjob1763328031961229584.jar tmpDir=null\n",
      "17/09/11 15:15:03 INFO client.AHSProxy: Connecting to Application History server at dscim003.palmetto.clemson.edu/10.125.8.215:10200\n",
      "17/09/11 15:15:03 INFO client.AHSProxy: Connecting to Application History server at dscim003.palmetto.clemson.edu/10.125.8.215:10200\n",
      "17/09/11 15:15:03 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 14249 for lngo on ha-hdfs:dsci\n",
      "17/09/11 15:15:03 INFO security.TokenCache: Got dt for hdfs://dsci; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:dsci, Ident: (HDFS_DELEGATION_TOKEN token 14249 for lngo)\n",
      "17/09/11 15:15:03 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library\n",
      "17/09/11 15:15:03 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 8787857212dae53ffae3b3113abc894e6743b4ab]\n",
      "17/09/11 15:15:03 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/09/11 15:15:04 INFO mapreduce.JobSubmitter: number of splits:5\n",
      "17/09/11 15:15:04 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1504719443162_0081\n",
      "17/09/11 15:15:04 INFO mapreduce.JobSubmitter: Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:dsci, Ident: (HDFS_DELEGATION_TOKEN token 14249 for lngo)\n",
      "17/09/11 15:15:04 INFO impl.TimelineClientImpl: Timeline service address: http://dscim003.palmetto.clemson.edu:8188/ws/v1/timeline/\n",
      "17/09/11 15:15:04 INFO impl.YarnClientImpl: Submitted application application_1504719443162_0081\n",
      "17/09/11 15:15:05 INFO mapreduce.Job: The url to track the job: http://dscim001.palmetto.clemson.edu:8088/proxy/application_1504719443162_0081/\n",
      "17/09/11 15:15:05 INFO mapreduce.Job: Running job: job_1504719443162_0081\n",
      "17/09/11 15:15:11 INFO mapreduce.Job: Job job_1504719443162_0081 running in uber mode : false\n",
      "17/09/11 15:15:11 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/09/11 15:15:23 INFO mapreduce.Job:  map 37% reduce 0%\n",
      "17/09/11 15:15:26 INFO mapreduce.Job:  map 58% reduce 0%\n",
      "17/09/11 15:15:29 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "17/09/11 15:15:32 INFO mapreduce.Job:  map 73% reduce 0%\n",
      "17/09/11 15:15:33 INFO mapreduce.Job:  map 87% reduce 0%\n",
      "17/09/11 15:15:34 INFO mapreduce.Job:  map 93% reduce 0%\n",
      "17/09/11 15:15:35 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/09/11 15:15:43 INFO mapreduce.Job:  map 100% reduce 44%\n",
      "17/09/11 15:15:46 INFO mapreduce.Job:  map 100% reduce 53%\n",
      "17/09/11 15:15:49 INFO mapreduce.Job:  map 100% reduce 61%\n",
      "17/09/11 15:15:53 INFO mapreduce.Job:  map 100% reduce 68%\n",
      "17/09/11 15:15:56 INFO mapreduce.Job:  map 100% reduce 71%\n",
      "17/09/11 15:15:59 INFO mapreduce.Job:  map 100% reduce 75%\n",
      "17/09/11 15:16:02 INFO mapreduce.Job:  map 100% reduce 79%\n",
      "17/09/11 15:16:05 INFO mapreduce.Job:  map 100% reduce 83%\n",
      "17/09/11 15:16:08 INFO mapreduce.Job:  map 100% reduce 88%\n",
      "17/09/11 15:16:11 INFO mapreduce.Job:  map 100% reduce 92%\n",
      "17/09/11 15:16:14 INFO mapreduce.Job:  map 100% reduce 96%\n",
      "17/09/11 15:16:17 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/09/11 15:16:17 INFO mapreduce.Job: Job job_1504719443162_0081 completed successfully\n",
      "17/09/11 15:16:17 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=778205542\n",
      "\t\tFILE: Number of bytes written=1557391447\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=663945432\n",
      "\t\tHDFS: Number of bytes written=1461512\n",
      "\t\tHDFS: Number of read operations=18\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=5\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tRack-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=302682\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=128430\n",
      "\t\tTotal time spent by all map tasks (ms)=100894\n",
      "\t\tTotal time spent by all reduce tasks (ms)=42810\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=100894\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=42810\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=1300725448\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=551906520\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=24404097\n",
      "\t\tMap output records=24404096\n",
      "\t\tMap output bytes=729396926\n",
      "\t\tMap output materialized bytes=778205566\n",
      "\t\tInput split bytes=480\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=39408\n",
      "\t\tReduce shuffle bytes=778205566\n",
      "\t\tReduce input records=24404096\n",
      "\t\tReduce output records=39408\n",
      "\t\tSpilled Records=48808192\n",
      "\t\tShuffled Maps =5\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=5\n",
      "\t\tGC time elapsed (ms)=3197\n",
      "\t\tCPU time spent (ms)=229150\n",
      "\t\tPhysical memory (bytes) snapshot=15842320384\n",
      "\t\tVirtual memory (bytes) snapshot=79770562560\n",
      "\t\tTotal committed heap usage (bytes)=17568366592\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=663944952\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1461512\n",
      "17/09/11 15:16:17 INFO streaming.StreamJob: Output directory: intro-to-hadoop/output-movielens-02\n"
     ]
    }
   ],
   "source": [
    "!ssh dsciutil hdfs dfs -rm -r intro-to-hadoop/output-movielens-02\n",
    "!ssh dsciutil yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input /repository/movielens/ratings.csv \\\n",
    "    -output intro-to-hadoop/output-movielens-02 \\\n",
    "    -file /home/lngo/git/intro-to-hadoop-python/avgRatingMapper04.py \\\n",
    "    -mapper avgRatingMapper04.py \\\n",
    "    -file /home/lngo/git/intro-to-hadoop-python/avgRatingReducer01.py \\\n",
    "    -reducer avgRatingReducer01.py \\\n",
    "    -file /home/lngo/git/intro-to-hadoop-python/movielens/movies.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is being passed from Map to Reduce?\n",
    "- Can reducer do the same thing as mapper, that is, to load in external data?\n",
    "- If we load external data on the reduce side, do we need to do so on the map side?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/avgRatingReducer02.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/avgRatingReducer02.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "movieFile = \"./movies.csv\"\n",
    "movieList = {}\n",
    "\n",
    "with open(movieFile, mode = 'r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    for row in reader:\n",
    "        movieList[row[0]] = {}\n",
    "        movieList[row[0]][\"title\"] = row[1]\n",
    "        movieList[row[0]][\"genre\"] = row[2]\n",
    "\n",
    "current_movie = None\n",
    "current_rating_sum = 0\n",
    "current_rating_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    movie, rating = line.split(\"\\t\", 1)\n",
    "    try:\n",
    "        rating = float(rating)\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    if current_movie == movie:\n",
    "        current_rating_sum += rating\n",
    "        current_rating_count += 1\n",
    "    else:\n",
    "        if current_movie:\n",
    "            rating_average = current_rating_sum / current_rating_count\n",
    "            movieTitle = movieList[current_movie][\"title\"]\n",
    "            movieGenres = movieList[current_movie][\"genre\"]\n",
    "            print (\"%s\\t%s\\t%s\" % (movieTitle, rating_average, movieGenres))    \n",
    "        current_movie = movie\n",
    "        current_rating_sum = rating\n",
    "        current_rating_count = 1\n",
    "\n",
    "if current_movie == movie:\n",
    "    rating_average = current_rating_sum / current_rating_count\n",
    "    movieTitle = movieList[current_movie][\"title\"]\n",
    "    movieGenres = movieList[current_movie][\"genre\"]\n",
    "    print (\"%s\\t%s\\t%s\" % (movieTitle, rating_average, movieGenres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!ssh dsciutil hdfs dfs -rm -r intro-to-hadoop/output-movielens-03\n",
    "!ssh dsciutil yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input intro-to-hadoop/movielens/ratings.csv \\\n",
    "    -output intro-to-hadoop/output-movielens-03 \\\n",
    "    -file /home/lngo/intro-to-hadoop/codes/avgRatingMapper02.py \\\n",
    "    -mapper avgRatingMapper02.py \\\n",
    "    -file /home/lngo/intro-to-hadoop/codes/avgRatingReducer02.py \\\n",
    "    -reducer avgRatingReducer02.py \\\n",
    "    -file /home/lngo/intro-to-hadoop/movielens/movies.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciutil hdfs dfs -ls intro-to-hadoop/output-movielens-03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciutil hdfs dfs -cat intro-to-hadoop/output-movielens-03/part-00000 \\\n",
    "    2>/dev/null | head -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the number shuffle bytes in this example compare to the previous example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find genres which have the highest average ratings over the years\n",
    "\n",
    "Common optimization approaches:\n",
    "\n",
    "1. In-mapper reduction of key/value pairs\n",
    "2. Additional combiner function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing codes/avgGenreMapper01.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/avgGenreMapper01.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "# for nonHDFS run\n",
    "# movieFile = \"./movielens/movies.csv\"\n",
    "\n",
    "# for HDFS run\n",
    "#movieFile = \"./movies.csv\"\n",
    "movieList = {}\n",
    "\n",
    "with open(movieFile, mode = 'r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    for row in reader:\n",
    "        movieList[row[0]] = {}\n",
    "        movieList[row[0]][\"title\"] = row[1]\n",
    "        movieList[row[0]][\"genre\"] = row[2]\n",
    "\n",
    "for oneMovie in sys.stdin:\n",
    "    oneMovie = oneMovie.strip()\n",
    "    ratingInfo = oneMovie.split(\",\")\n",
    "    try:\n",
    "        genreList = movieList[ratingInfo[1]][\"genre\"]\n",
    "        rating = float(ratingInfo[2])\n",
    "        for genre in genreList.split(\"|\"):\n",
    "            print (\"%s\\t%s\" % (genre, rating))\n",
    "    except ValueError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing codes/avgGenreReducer01.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/avgGenreReducer01.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import csv\n",
    "import json\n",
    "\n",
    "current_genre = None\n",
    "current_rating_sum = 0\n",
    "current_rating_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    genre, ratingString = line.split(\"\\t\", 1)\n",
    "    ratingInfo = json.loads(ratingString)\n",
    "\n",
    "    if current_genre == genre:\n",
    "        try:\n",
    "            current_rating_sum += ratingInfo[\"total_rating\"]\n",
    "            current_rating_count += ratingInfo[\"total_count\"]\n",
    "        except ValueError:\n",
    "            continue    \n",
    "    else:\n",
    "        if current_genre:\n",
    "            rating_average = current_rating_sum / current_rating_count\n",
    "            print (\"%s\\t%s\" % (current_genre, rating_average))    \n",
    "        current_genre = genre\n",
    "        try:\n",
    "            current_rating_sum = ratingInfo[\"total_rating\"]\n",
    "            current_rating_count = ratingInfo[\"total_count\"]\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "if current_genre == genre:\n",
    "    rating_average = current_rating_sum / current_rating_count\n",
    "    print (\"%s\\t%s\" % (current_genre, rating_average))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciutil hdfs dfs -rm -r intro-to-hadoop/output-movielens-04\n",
    "!ssh dsciutil yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input intro-to-hadoop/movielens/ratings.csv \\\n",
    "    -output intro-to-hadoop/output-movielens-04 \\\n",
    "    -file /home/lngo/intro-to-hadoop/codes/avgGenreMapper01.py \\\n",
    "    -mapper avgGenreMapper01.py \\\n",
    "    -file /home/lngo/intro-to-hadoop/codes/avgGenreReducer01.py \\\n",
    "    -reducer avgRatingReducer01.py \\\n",
    "    -file /home/lngo/intro-to-hadoop/movielens/movies.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciutil hdfs dfs -ls intro-to-hadoop/output-movielens-04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciutil hdfs dfs -cat intro-to-hadoop/output-movielens-04/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Optimization through in-mapper reduction of Key/Value pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId,movieId,rating,timestamp\r",
      "\r\n",
      "1,122,2.0,945544824\r",
      "\r\n",
      "1,172,1.0,945544871\r",
      "\r\n",
      "1,1221,5.0,945544788\r",
      "\r\n",
      "1,1441,4.0,945544871\r",
      "\r\n",
      "1,1609,3.0,945544824\r",
      "\r\n",
      "1,1961,3.0,945544871\r",
      "\r\n",
      "1,1972,1.0,945544871\r",
      "\r\n",
      "2,441,2.0,1008942733\r",
      "\r\n",
      "2,494,2.0,1008942733\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!ssh dsciutil hdfs dfs -cat intro-to-hadoop/movielens/ratings.csv 2>/dev/null \\\n",
    "    | head -n 10 \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comedy\t2.0\r\n",
      "Romance\t2.0\r\n",
      "Action\t1.0\r\n",
      "Sci-Fi\t1.0\r\n",
      "Thriller\t1.0\r\n",
      "Crime\t5.0\r\n",
      "Drama\t5.0\r\n",
      "Comedy\t4.0\r\n",
      "Romance\t4.0\r\n",
      "Drama\t3.0\r\n",
      "Thriller\t3.0\r\n",
      "Drama\t3.0\r\n",
      "Horror\t1.0\r\n",
      "Comedy\t2.0\r\n",
      "Action\t2.0\r\n",
      "Adventure\t2.0\r\n",
      "Thriller\t2.0\r\n"
     ]
    }
   ],
   "source": [
    "!ssh dsciutil hdfs dfs -cat intro-to-hadoop/movielens/ratings.csv 2>/dev/null \\\n",
    "    | head -n 10 \\\n",
    "    | python /home/lngo/intro-to-hadoop/avgGenreMapper01.py \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/avgGenreMapper02.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/avgGenreMapper02.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "import json\n",
    "\n",
    "# for nonHDFS run\n",
    "# movieFile = \"./movielens/movies.csv\"\n",
    "\n",
    "# for HDFS run\n",
    "movieFile = \"./movies.csv\"\n",
    "\n",
    "movieList = {}\n",
    "genreList = {}\n",
    "\n",
    "with open(movieFile, mode = 'r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    for row in reader:\n",
    "        movieList[row[0]] = {}\n",
    "        movieList[row[0]][\"title\"] = row[1]\n",
    "        movieList[row[0]][\"genre\"] = row[2]\n",
    "\n",
    "for oneMovie in sys.stdin:\n",
    "    oneMovie = oneMovie.strip()\n",
    "    ratingInfo = oneMovie.split(\",\")\n",
    "    try:\n",
    "        genres = movieList[ratingInfo[1]][\"genre\"]\n",
    "        rating = float(ratingInfo[2])\n",
    "        for genre in genres.split(\"|\"):\n",
    "            if genre in genreList:\n",
    "                genreList[genre][\"total_rating\"] += rating\n",
    "                genreList[genre][\"total_count\"] += 1\n",
    "            else:\n",
    "                genreList[genre] = {}\n",
    "                genreList[genre][\"total_rating\"] = rating\n",
    "                genreList[genre][\"total_count\"] = 1\n",
    "    except ValueError:\n",
    "        continue\n",
    "        \n",
    "for genre in genreList:\n",
    "    print (\"%s\\t%s\" % (genre, json.dumps(genreList[genre])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sci-Fi\t{\"total_rating\": 1.0, \"total_count\": 1}\r\n",
      "Romance\t{\"total_rating\": 6.0, \"total_count\": 2}\r\n",
      "Thriller\t{\"total_rating\": 6.0, \"total_count\": 3}\r\n",
      "Drama\t{\"total_rating\": 11.0, \"total_count\": 3}\r\n",
      "Comedy\t{\"total_rating\": 8.0, \"total_count\": 3}\r\n",
      "Adventure\t{\"total_rating\": 2.0, \"total_count\": 1}\r\n",
      "Action\t{\"total_rating\": 3.0, \"total_count\": 2}\r\n",
      "Horror\t{\"total_rating\": 1.0, \"total_count\": 1}\r\n",
      "Crime\t{\"total_rating\": 5.0, \"total_count\": 1}\r\n"
     ]
    }
   ],
   "source": [
    "!ssh dsciutil hdfs dfs -cat intro-to-hadoop/movielens/ratings.csv 2>/dev/null \\\n",
    "    | head -n 10 \\\n",
    "    | python /home/lngo/git/intro-to-hadoop-python/avgGenreMapper02.py \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action\t1.5\r\n",
      "Adventure\t2.0\r\n",
      "Comedy\t2.6666666666666665\r\n",
      "Crime\t5.0\r\n",
      "Drama\t3.6666666666666665\r\n",
      "Horror\t1.0\r\n",
      "Romance\t3.0\r\n",
      "Sci-Fi\t1.0\r\n",
      "Thriller\t2.0\r\n"
     ]
    }
   ],
   "source": [
    "!ssh dsciutil hdfs dfs -cat intro-to-hadoop/movielens/ratings.csv 2>/dev/null \\\n",
    "    | head -n 10 \\\n",
    "    | python /home/lngo/git/intro-to-hadoop-python/avgGenreMapper02.py \\\n",
    "    | sort \\\n",
    "    | python /home/lngo/git/intro-to-hadoop-python/avgGenreReducer01.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/09/11 15:33:39 INFO fs.TrashPolicyDefault: Moved: 'hdfs://dsci/user/lngo/intro-to-hadoop/output-movielens-05' to trash at: hdfs://dsci/user/lngo/.Trash/Current/user/lngo/intro-to-hadoop/output-movielens-051505158419435\n",
      "17/09/11 15:33:41 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/home/lngo/git/intro-to-hadoop-python/codes/avgGenreMapper02.py, /home/lngo/git/intro-to-hadoop-python/codes/avgGenreReducer01.py, /home/lngo/git/intro-to-hadoop-python/movielens/movies.csv] [/usr/hdp/2.6.0.3-8/hadoop-mapreduce/hadoop-streaming-2.7.3.2.6.0.3-8.jar] /hadoop_java_io_tmpdir/streamjob2129448351179621896.jar tmpDir=null\n",
      "17/09/11 15:33:42 INFO client.AHSProxy: Connecting to Application History server at dscim003.palmetto.clemson.edu/10.125.8.215:10200\n",
      "17/09/11 15:33:42 INFO client.AHSProxy: Connecting to Application History server at dscim003.palmetto.clemson.edu/10.125.8.215:10200\n",
      "17/09/11 15:33:42 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 14252 for lngo on ha-hdfs:dsci\n",
      "17/09/11 15:33:42 INFO security.TokenCache: Got dt for hdfs://dsci; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:dsci, Ident: (HDFS_DELEGATION_TOKEN token 14252 for lngo)\n",
      "17/09/11 15:33:43 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library\n",
      "17/09/11 15:33:43 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 8787857212dae53ffae3b3113abc894e6743b4ab]\n",
      "17/09/11 15:33:43 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/09/11 15:33:43 INFO mapreduce.JobSubmitter: number of splits:5\n",
      "17/09/11 15:33:43 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1504719443162_0084\n",
      "17/09/11 15:33:43 INFO mapreduce.JobSubmitter: Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:dsci, Ident: (HDFS_DELEGATION_TOKEN token 14252 for lngo)\n",
      "17/09/11 15:33:44 INFO impl.TimelineClientImpl: Timeline service address: http://dscim003.palmetto.clemson.edu:8188/ws/v1/timeline/\n",
      "17/09/11 15:33:44 INFO impl.YarnClientImpl: Submitted application application_1504719443162_0084\n",
      "17/09/11 15:33:44 INFO mapreduce.Job: The url to track the job: http://dscim001.palmetto.clemson.edu:8088/proxy/application_1504719443162_0084/\n",
      "17/09/11 15:33:44 INFO mapreduce.Job: Running job: job_1504719443162_0084\n",
      "17/09/11 15:33:51 INFO mapreduce.Job: Job job_1504719443162_0084 running in uber mode : false\n",
      "17/09/11 15:33:51 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/09/11 15:34:02 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "17/09/11 15:34:05 INFO mapreduce.Job:  map 47% reduce 0%\n",
      "17/09/11 15:34:08 INFO mapreduce.Job:  map 85% reduce 0%\n",
      "17/09/11 15:34:09 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/09/11 15:34:14 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/09/11 15:34:15 INFO mapreduce.Job: Job job_1504719443162_0084 completed successfully\n",
      "17/09/11 15:34:15 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=6105\n",
      "\t\tFILE: Number of bytes written=992687\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=663945507\n",
      "\t\tHDFS: Number of bytes written=447\n",
      "\t\tHDFS: Number of read operations=18\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=5\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tRack-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=239370\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=11580\n",
      "\t\tTotal time spent by all map tasks (ms)=79790\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3860\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=79790\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3860\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=1028652680\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=49763120\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=24404097\n",
      "\t\tMap output records=100\n",
      "\t\tMap output bytes=5899\n",
      "\t\tMap output materialized bytes=6129\n",
      "\t\tInput split bytes=555\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=20\n",
      "\t\tReduce shuffle bytes=6129\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=20\n",
      "\t\tSpilled Records=200\n",
      "\t\tShuffled Maps =5\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=5\n",
      "\t\tGC time elapsed (ms)=2817\n",
      "\t\tCPU time spent (ms)=141360\n",
      "\t\tPhysical memory (bytes) snapshot=13886418944\n",
      "\t\tVirtual memory (bytes) snapshot=79767678976\n",
      "\t\tTotal committed heap usage (bytes)=15892217856\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=663944952\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=447\n",
      "17/09/11 15:34:15 INFO streaming.StreamJob: Output directory: intro-to-hadoop/output-movielens-05\n"
     ]
    }
   ],
   "source": [
    "# make sure that the path to movies.csv is correct inside avgGenreMapper02.py\n",
    "!ssh dsciutil hdfs dfs -rm -R intro-to-hadoop/output-movielens-05\n",
    "!ssh dsciutil yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input intro-to-hadoop/movielens/ratings.csv \\\n",
    "    -output intro-to-hadoop/output-movielens-05 \\\n",
    "    -file /home/lngo/git/intro-to-hadoop-python/codes/avgGenreMapper02.py \\\n",
    "    -mapper avgGenreMapper02.py \\\n",
    "    -file /home/lngo/git/intro-to-hadoop-python/codes/avgGenreReducer01.py \\\n",
    "    -reducer avgGenreReducer01.py \\\n",
    "    -file /home/lngo/git/intro-to-hadoop-python/movielens/movies.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(no genres listed)\t3.20801494311\r\n",
      "Action\t3.45445315141\r\n",
      "Adventure\t3.50709193718\r\n",
      "Animation\t3.61049896425\r\n",
      "Children\t3.41664063098\r\n",
      "Comedy\t3.41746035479\r\n",
      "Crime\t3.67850196299\r\n",
      "Documentary\t3.72277231013\r\n",
      "Drama\t3.67427737349\r\n",
      "Fantasy\t3.50299123143\r\n",
      "Film-Noir\t3.9408055354\r\n",
      "Horror\t3.27526021431\r\n",
      "IMAX\t3.63709765837\r\n",
      "Musical\t3.54391359231\r\n",
      "Mystery\t3.66150976856\r\n",
      "Romance\t3.54246199954\r\n",
      "Sci-Fi\t3.45517283887\r\n",
      "Thriller\t3.51269029345\r\n",
      "War\t3.80326678256\r\n",
      "Western\t3.57161948559\r\n"
     ]
    }
   ],
   "source": [
    "!ssh dsciutil hdfs dfs -cat intro-to-hadoop/output-movielens-05/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How different are the number of shuffle bytes between the two jobs?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Optimization through combiner function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!ssh dsciutil yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input intro-to-hadoop/text/gutenberg-shakespeare.txt \\\n",
    "    -output intro-to-hadoop/output-wordcount-01 \\\n",
    "    -file /home/lngo/git/intro-to-hadoop-python/codes/wordcountMapper.py \\\n",
    "    -mapper wordcountMapper.py \\\n",
    "    -file /home/lngo/git/intro-to-hadoop-python/codes/wordcountReducer.py \\\n",
    "    -reducer wordcountReducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!ssh dsciutil yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input intro-to-hadoop/text/gutenberg-shakespeare.txt \\\n",
    "    -output intro-to-hadoop/output-wordcount-02 \\\n",
    "    -file /home/lngo/git/intro-to-hadoop-python/codes/wordcountMapper.py \\\n",
    "    -mapper wordcountMapper.py \\\n",
    "    -file /home/lngo/git/intro-to-hadoop-python/codes/wordcountReducer.py \\\n",
    "    -reducer wordcountReducer.py \\\n",
    "    -combiner wordcountReducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing codes/avgGenreCombiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/avgGenreCombiner.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "import json\n",
    "\n",
    "genreList = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    genre, ratingString = line.split(\"\\t\", 1)\n",
    "    ratingInfo = json.loads(ratingString)\n",
    "\n",
    "    if genre in genreList:\n",
    "        genreList[genre][\"total_rating\"] += ratingInfo[\"total_rating\"]\n",
    "        genreList[genre][\"total_count\"] += ratingInfo[\"total_count\"]\n",
    "    else:\n",
    "        genreList[genre] = {}\n",
    "        genreList[genre][\"total_rating\"] = ratingInfo[\"total_rating\"]\n",
    "        genreList[genre][\"total_count\"] = 1\n",
    "\n",
    "for genre in genreList:\n",
    "    print (\"%s\\t%s\" % (genre, json.dumps(genreList[genre])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `intro-to-hadoop/output-movielens-06': No such file or directory\n",
      "17/09/11 15:36:14 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/home/lngo/git/intro-to-hadoop-python/codes/avgGenreMapper02.py, /home/lngo/git/intro-to-hadoop-python/codes/avgGenreReducer01.py, /home/lngo/git/intro-to-hadoop-python/codes/avgGenreCombiner.py, /home/lngo/git/intro-to-hadoop-python/movielens/movies.csv] [/usr/hdp/2.6.0.3-8/hadoop-mapreduce/hadoop-streaming-2.7.3.2.6.0.3-8.jar] /hadoop_java_io_tmpdir/streamjob5344462887892835959.jar tmpDir=null\n",
      "17/09/11 15:36:15 INFO client.AHSProxy: Connecting to Application History server at dscim003.palmetto.clemson.edu/10.125.8.215:10200\n",
      "17/09/11 15:36:15 INFO client.AHSProxy: Connecting to Application History server at dscim003.palmetto.clemson.edu/10.125.8.215:10200\n",
      "17/09/11 15:36:16 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 14253 for lngo on ha-hdfs:dsci\n",
      "17/09/11 15:36:16 INFO security.TokenCache: Got dt for hdfs://dsci; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:dsci, Ident: (HDFS_DELEGATION_TOKEN token 14253 for lngo)\n",
      "17/09/11 15:36:16 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library\n",
      "17/09/11 15:36:16 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 8787857212dae53ffae3b3113abc894e6743b4ab]\n",
      "17/09/11 15:36:16 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/09/11 15:36:16 INFO mapreduce.JobSubmitter: number of splits:5\n",
      "17/09/11 15:36:16 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1504719443162_0085\n",
      "17/09/11 15:36:16 INFO mapreduce.JobSubmitter: Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:dsci, Ident: (HDFS_DELEGATION_TOKEN token 14253 for lngo)\n",
      "17/09/11 15:36:17 INFO impl.TimelineClientImpl: Timeline service address: http://dscim003.palmetto.clemson.edu:8188/ws/v1/timeline/\n",
      "17/09/11 15:36:17 INFO impl.YarnClientImpl: Submitted application application_1504719443162_0085\n",
      "17/09/11 15:36:17 INFO mapreduce.Job: The url to track the job: http://dscim001.palmetto.clemson.edu:8088/proxy/application_1504719443162_0085/\n",
      "17/09/11 15:36:17 INFO mapreduce.Job: Running job: job_1504719443162_0085\n",
      "17/09/11 15:36:23 INFO mapreduce.Job: Job job_1504719443162_0085 running in uber mode : false\n",
      "17/09/11 15:36:23 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/09/11 15:36:36 INFO mapreduce.Job:  map 13% reduce 0%\n",
      "17/09/11 15:36:39 INFO mapreduce.Job:  map 20% reduce 0%\n",
      "17/09/11 15:36:41 INFO mapreduce.Job:  map 47% reduce 0%\n",
      "17/09/11 15:36:42 INFO mapreduce.Job:  map 57% reduce 0%\n",
      "17/09/11 15:36:44 INFO mapreduce.Job:  map 65% reduce 0%\n",
      "17/09/11 15:36:47 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "17/09/11 15:36:48 INFO mapreduce.Job:  map 92% reduce 0%\n",
      "17/09/11 15:36:49 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/09/11 15:36:50 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/09/11 15:36:50 INFO mapreduce.Job: Job job_1504719443162_0085 completed successfully\n",
      "17/09/11 15:36:50 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5603\n",
      "\t\tFILE: Number of bytes written=995595\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=663945507\n",
      "\t\tHDFS: Number of bytes written=1653\n",
      "\t\tHDFS: Number of read operations=18\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=6\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tRack-local map tasks=4\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=307839\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=17448\n",
      "\t\tTotal time spent by all map tasks (ms)=102613\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5816\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=102613\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5816\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=1322886796\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=74979872\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=24404097\n",
      "\t\tMap output records=100\n",
      "\t\tMap output bytes=5899\n",
      "\t\tMap output materialized bytes=5627\n",
      "\t\tInput split bytes=555\n",
      "\t\tCombine input records=100\n",
      "\t\tCombine output records=100\n",
      "\t\tReduce input groups=92\n",
      "\t\tReduce shuffle bytes=5627\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=92\n",
      "\t\tSpilled Records=200\n",
      "\t\tShuffled Maps =5\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=5\n",
      "\t\tGC time elapsed (ms)=2838\n",
      "\t\tCPU time spent (ms)=140670\n",
      "\t\tPhysical memory (bytes) snapshot=14240346112\n",
      "\t\tVirtual memory (bytes) snapshot=79759732736\n",
      "\t\tTotal committed heap usage (bytes)=16041115648\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=663944952\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1653\n",
      "17/09/11 15:36:50 INFO streaming.StreamJob: Output directory: intro-to-hadoop/output-movielens-06\n"
     ]
    }
   ],
   "source": [
    "!ssh dsciutil hdfs dfs -rm -r intro-to-hadoop/output-movielens-06\n",
    "!ssh dsciutil yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input intro-to-hadoop/movielens/ratings.csv \\\n",
    "    -output intro-to-hadoop/output-movielens-06 \\\n",
    "    -file /home/lngo/git/intro-to-hadoop-python/codes/avgGenreMapper02.py \\\n",
    "    -mapper avgGenreMapper02.py \\\n",
    "    -file /home/lngo/git/intro-to-hadoop-python/codes/avgGenreReducer01.py \\\n",
    "    -reducer avgGenreReducer01.py \\\n",
    "    -file /home/lngo/git/intro-to-hadoop-python/codes/avgGenreCombiner.py \\\n",
    "    -combiner avgGenreCombiner.py \\\n",
    "    -file /home/lngo/git/intro-to-hadoop-python/movielens/movies.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How different are the number of shuffle bytes between the two jobs?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <center> Final Cleanup </center>\n",
    "\n",
    "Executing the cell below will clean up all HDFS output directories created as a result of previous MapReduce programs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 items\r\n",
      "drwxr-xr-x   - lngo hdfs          0 2017-04-13 14:53 intro-to-hadoop/movielens\r\n",
      "drwxr-xr-x   - lngo hdfs          0 2017-04-13 15:07 intro-to-hadoop/output-movielens-01\r\n",
      "drwxr-xr-x   - lngo hdfs          0 2017-04-13 15:25 intro-to-hadoop/output-movielens-02\r\n",
      "drwxr-xr-x   - lngo hdfs          0 2017-04-13 15:29 intro-to-hadoop/output-movielens-03\r\n",
      "drwxr-xr-x   - lngo hdfs          0 2017-04-13 15:36 intro-to-hadoop/output-movielens-04\r\n",
      "drwxr-xr-x   - lngo hdfs          0 2017-04-13 15:46 intro-to-hadoop/output-movielens-05\r\n",
      "drwxr-xr-x   - lngo hdfs          0 2017-04-13 15:54 intro-to-hadoop/output-movielens-06\r\n",
      "drwxr-xr-x   - lngo hdfs          0 2017-04-13 15:49 intro-to-hadoop/output-wordcount-01\r\n",
      "drwxr-xr-x   - lngo hdfs          0 2017-04-13 15:51 intro-to-hadoop/output-wordcount-02\r\n",
      "drwxr-xr-x   - lngo hdfs          0 2016-10-17 14:01 intro-to-hadoop/text\r\n"
     ]
    }
   ],
   "source": [
    "!ssh dsciutil hdfs dfs -ls intro-to-hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `intro-to-hadoop/output-wordcount': No such file or directory\n",
      "rm: `intro-to-hadoop/output-wordcount-01': No such file or directory\n",
      "rm: `intro-to-hadoop/output-wordcount-02': No such file or directory\n",
      "rm: `intro-to-hadoop/output-movielens-01': No such file or directory\n",
      "17/09/11 15:38:33 INFO fs.TrashPolicyDefault: Moved: 'hdfs://dsci/user/lngo/intro-to-hadoop/output-movielens-02' to trash at: hdfs://dsci/user/lngo/.Trash/Current/user/lngo/intro-to-hadoop/output-movielens-021505158713772\n",
      "rm: `intro-to-hadoop/output-movielens-03': No such file or directory\n",
      "rm: `intro-to-hadoop/output-movielens-04': No such file or directory\n",
      "17/09/11 15:38:42 INFO fs.TrashPolicyDefault: Moved: 'hdfs://dsci/user/lngo/intro-to-hadoop/output-movielens-05' to trash at: hdfs://dsci/user/lngo/.Trash/Current/user/lngo/intro-to-hadoop/output-movielens-051505158722193\n",
      "17/09/11 15:38:45 INFO fs.TrashPolicyDefault: Moved: 'hdfs://dsci/user/lngo/intro-to-hadoop/output-movielens-06' to trash at: hdfs://dsci/user/lngo/.Trash/Current/user/lngo/intro-to-hadoop/output-movielens-06\n"
     ]
    }
   ],
   "source": [
    "!ssh dsciutil hdfs dfs -rm -r intro-to-hadoop/output-wordcount\n",
    "!ssh dsciutil hdfs dfs -rm -r intro-to-hadoop/output-wordcount-01\n",
    "!ssh dsciutil hdfs dfs -rm -r intro-to-hadoop/output-wordcount-02\n",
    "!ssh dsciutil hdfs dfs -rm -r intro-to-hadoop/output-movielens-01\n",
    "!ssh dsciutil hdfs dfs -rm -r intro-to-hadoop/output-movielens-02\n",
    "!ssh dsciutil hdfs dfs -rm -r intro-to-hadoop/output-movielens-03\n",
    "!ssh dsciutil hdfs dfs -rm -r intro-to-hadoop/output-movielens-04\n",
    "!ssh dsciutil hdfs dfs -rm -r intro-to-hadoop/output-movielens-05\n",
    "!ssh dsciutil hdfs dfs -rm -r intro-to-hadoop/output-movielens-06\n",
    "!rm -Rf codes/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 (Anaconda)",
   "language": "python",
   "name": "anaconda_py2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
