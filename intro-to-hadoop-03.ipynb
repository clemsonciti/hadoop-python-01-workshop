{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Introduction to Hadoop MapReduce </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First principle of optimizing Hadoop workflow: **Reduce data movement in the shuffle phase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r intro-to-hadoop/output-movielens-02\n",
    "!yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input /repository/movielens/ratings.csv \\\n",
    "    -output intro-to-hadoop/output-movielens-02 \\\n",
    "    -file ./codes/avgRatingMapper04.py \\\n",
    "    -mapper avgRatingMapper04.py \\\n",
    "    -file ./codes/avgRatingReducer01.py \\\n",
    "    -reducer avgRatingReducer01.py \\\n",
    "    -file ./movielens/movies.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is being passed from Map to Reduce?\n",
    "- Can reducer do the same thing as mapper, that is, to load in external data?\n",
    "- If we load external data on the reduce side, do we need to do so on the map side?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile codes/avgRatingReducer02.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "movieFile = \"./movies.csv\"\n",
    "movieList = {}\n",
    "\n",
    "with open(movieFile, mode = 'r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    for row in reader:\n",
    "        movieList[row[0]] = {}\n",
    "        movieList[row[0]][\"title\"] = row[1]\n",
    "        movieList[row[0]][\"genre\"] = row[2]\n",
    "\n",
    "current_movie = None\n",
    "current_rating_sum = 0\n",
    "current_rating_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    movie, rating = line.split(\"\\t\", 1)\n",
    "    try:\n",
    "        rating = float(rating)\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    if current_movie == movie:\n",
    "        current_rating_sum += rating\n",
    "        current_rating_count += 1\n",
    "    else:\n",
    "        if current_movie:\n",
    "            rating_average = current_rating_sum / current_rating_count\n",
    "            movieTitle = movieList[current_movie][\"title\"]\n",
    "            movieGenres = movieList[current_movie][\"genre\"]\n",
    "            print (\"%s\\t%s\\t%s\" % (movieTitle, rating_average, movieGenres))    \n",
    "        current_movie = movie\n",
    "        current_rating_sum = rating\n",
    "        current_rating_count = 1\n",
    "\n",
    "if current_movie == movie:\n",
    "    rating_average = current_rating_sum / current_rating_count\n",
    "    movieTitle = movieList[current_movie][\"title\"]\n",
    "    movieGenres = movieList[current_movie][\"genre\"]\n",
    "    print (\"%s\\t%s\\t%s\" % (movieTitle, rating_average, movieGenres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r intro-to-hadoop/output-movielens-03\n",
    "!yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input /repository/movielens/ratings.csv \\\n",
    "    -output intro-to-hadoop/output-movielens-03 \\\n",
    "    -file ./codes/avgRatingMapper02.py \\\n",
    "    -mapper avgRatingMapper02.py \\\n",
    "    -file ./codes/avgRatingReducer02.py \\\n",
    "    -reducer avgRatingReducer02.py \\\n",
    "    -file ./movielens/movies.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -ls intro-to-hadoop/output-movielens-02\n",
    "!hdfs dfs -ls intro-to-hadoop/output-movielens-03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat intro-to-hadoop/output-movielens-03/part-00000 \\\n",
    "    2>/dev/null | head -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the number shuffle bytes in this example compare to the previous example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find genres which have the highest average ratings over the years\n",
    "\n",
    "Common optimization approaches:\n",
    "\n",
    "1. In-mapper reduction of key/value pairs\n",
    "2. Additional combiner function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile codes/avgGenreMapper01.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "# for nonHDFS run\n",
    "movieFile = \"./movielens/movies.csv\"\n",
    "\n",
    "# for HDFS run\n",
    "#movieFile = \"./movies.csv\"\n",
    "movieList = {}\n",
    "\n",
    "with open(movieFile, mode = 'r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    for row in reader:\n",
    "        movieList[row[0]] = {}\n",
    "        movieList[row[0]][\"title\"] = row[1]\n",
    "        movieList[row[0]][\"genre\"] = row[2]\n",
    "\n",
    "for oneMovie in sys.stdin:\n",
    "    oneMovie = oneMovie.strip()\n",
    "    ratingInfo = oneMovie.split(\",\")\n",
    "    try:\n",
    "        genreList = movieList[ratingInfo[1]][\"genre\"]\n",
    "        rating = float(ratingInfo[2])\n",
    "        for genre in genreList.split(\"|\"):\n",
    "            print (\"%s\\t%s\" % (genre, rating))\n",
    "    except ValueError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile codes/avgGenreReducer01.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import csv\n",
    "import json\n",
    "\n",
    "current_genre = None\n",
    "current_rating_sum = 0\n",
    "current_rating_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    genre, rating = line.split(\"\\t\", 1)\n",
    "\n",
    "    if current_genre == genre:\n",
    "        try:\n",
    "            current_rating_sum += float(rating)\n",
    "            current_rating_count += 1\n",
    "        except ValueError:\n",
    "            continue    \n",
    "    else:\n",
    "        if current_genre:\n",
    "            rating_average = current_rating_sum / current_rating_count\n",
    "            print (\"%s\\t%s\" % (current_genre, rating_average))    \n",
    "        current_genre = genre\n",
    "        try:\n",
    "            current_rating_sum = float(rating)\n",
    "            current_rating_count = 1\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "if current_genre == genre:\n",
    "    rating_average = current_rating_sum / current_rating_count\n",
    "    print (\"%s\\t%s\" % (current_genre, rating_average))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r intro-to-hadoop/output-movielens-04\n",
    "!yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input /repository/movielens/ratings.csv \\\n",
    "    -output intro-to-hadoop/output-movielens-04 \\\n",
    "    -file ./codes/avgGenreMapper01.py \\\n",
    "    -mapper avgGenreMapper01.py \\\n",
    "    -file ./codes/avgGenreReducer01.py \\\n",
    "    -reducer avgGenreReducer01.py \\\n",
    "    -file ./movielens/movies.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -ls intro-to-hadoop/output-movielens-04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat intro-to-hadoop/output-movielens-04/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Optimization through in-mapper reduction of Key/Value pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat /repository/movielens/ratings.csv 2>/dev/null \\\n",
    "    | head -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cat /repository/movielens/ratings.csv 2>/dev/null \\\n",
    "    | head -n 10 \\\n",
    "    | python ./codes/avgGenreMapper01.py \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile codes/avgGenreMapper02.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "import json\n",
    "\n",
    "# for nonHDFS run\n",
    "# movieFile = \"./movielens/movies.csv\"\n",
    "\n",
    "# for HDFS run\n",
    "movieFile = \"./movies.csv\"\n",
    "\n",
    "movieList = {}\n",
    "genreList = {}\n",
    "\n",
    "with open(movieFile, mode = 'r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    for row in reader:\n",
    "        movieList[row[0]] = {}\n",
    "        movieList[row[0]][\"title\"] = row[1]\n",
    "        movieList[row[0]][\"genre\"] = row[2]\n",
    "\n",
    "for oneMovie in sys.stdin:\n",
    "    oneMovie = oneMovie.strip()\n",
    "    ratingInfo = oneMovie.split(\",\")\n",
    "    try:\n",
    "        genres = movieList[ratingInfo[1]][\"genre\"]\n",
    "        rating = float(ratingInfo[2])\n",
    "        for genre in genres.split(\"|\"):\n",
    "            if genre in genreList:\n",
    "                genreList[genre][\"total_rating\"] += rating\n",
    "                genreList[genre][\"total_count\"] += 1\n",
    "            else:\n",
    "                genreList[genre] = {}\n",
    "                genreList[genre][\"total_rating\"] = rating\n",
    "                genreList[genre][\"total_count\"] = 1\n",
    "    except ValueError:\n",
    "        continue\n",
    "        \n",
    "for genre in genreList:\n",
    "    print (\"%s\\t%s\" % (genre, json.dumps(genreList[genre])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat /repository/movielens/ratings.csv 2>/dev/null \\\n",
    "    | head -n 10 \\\n",
    "    | python ./codes/avgGenreMapper02.py \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile codes/avgGenreReducer02.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import csv\n",
    "import json\n",
    "\n",
    "current_genre = None\n",
    "current_rating_sum = 0\n",
    "current_rating_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    genre, ratingString = line.split(\"\\t\", 1)\n",
    "    ratingInfo = json.loads(ratingString)\n",
    "\n",
    "    if current_genre == genre:\n",
    "        try:\n",
    "            current_rating_sum += ratingInfo[\"total_rating\"]\n",
    "            current_rating_count += ratingInfo[\"total_count\"]\n",
    "        except ValueError:\n",
    "            continue    \n",
    "    else:\n",
    "        if current_genre:\n",
    "            rating_average = current_rating_sum / current_rating_count\n",
    "            print (\"%s\\t%s\" % (current_genre, rating_average))    \n",
    "        current_genre = genre\n",
    "        try:\n",
    "            current_rating_sum = ratingInfo[\"total_rating\"]\n",
    "            current_rating_count = ratingInfo[\"total_count\"]\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "if current_genre == genre:\n",
    "    rating_average = current_rating_sum / current_rating_count\n",
    "    print (\"%s\\t%s\" % (current_genre, rating_average))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat /repository/movielens/ratings.csv 2>/dev/null \\\n",
    "    | head -n 10 \\\n",
    "    | python ./codes/avgGenreMapper02.py \\\n",
    "    | sort \\\n",
    "    | python ./codes/avgGenreReducer02.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# make sure that the path to movies.csv is correct inside avgGenreMapper02.py\n",
    "!hdfs dfs -rm -R intro-to-hadoop/output-movielens-05\n",
    "!yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input /repository/movielens/ratings.csv \\\n",
    "    -output intro-to-hadoop/output-movielens-05 \\\n",
    "    -file ./codes/avgGenreMapper02.py \\\n",
    "    -mapper avgGenreMapper02.py \\\n",
    "    -file ./codes/avgGenreReducer02.py \\\n",
    "    -reducer avgGenreReducer02.py \\\n",
    "    -file ./movielens/movies.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat intro-to-hadoop/output-movielens-05/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cat intro-to-hadoop/output-movielens-04/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How different are the number of shuffle bytes between the two jobs?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Optimization through combiner function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input /repository/text/complete-shakespeare.txt \\\n",
    "    -output intro-to-hadoop/output-wordcount-01 \\\n",
    "    -file ./codes/wordcountMapper.py \\\n",
    "    -mapper wordcountMapper.py \\\n",
    "    -file ./codes/wordcountReducer.py \\\n",
    "    -reducer wordcountReducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input /repository/text/complete-shakespeare.txt \\\n",
    "    -output intro-to-hadoop/output-wordcount-02 \\\n",
    "    -file ./codes/wordcountMapper.py \\\n",
    "    -mapper wordcountMapper.py \\\n",
    "    -file ./codes/wordcountReducer.py \\\n",
    "    -reducer wordcountReducer.py \\\n",
    "    -combiner wordcountReducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile codes/avgGenreCombiner.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "import json\n",
    "\n",
    "genreList = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    genre, ratingString = line.split(\"\\t\", 1)\n",
    "    ratingInfo = json.loads(ratingString)\n",
    "\n",
    "    if genre in genreList:\n",
    "        genreList[genre][\"total_rating\"] += ratingInfo[\"total_rating\"]\n",
    "        genreList[genre][\"total_count\"] += ratingInfo[\"total_count\"]\n",
    "    else:\n",
    "        genreList[genre] = {}\n",
    "        genreList[genre][\"total_rating\"] = ratingInfo[\"total_rating\"]\n",
    "        genreList[genre][\"total_count\"] = 1\n",
    "\n",
    "for genre in genreList:\n",
    "    print (\"%s\\t%s\" % (genre, json.dumps(genreList[genre])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r intro-to-hadoop/output-movielens-06\n",
    "!yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input /repository/movielens/ratings.csv \\\n",
    "    -output intro-to-hadoop/output-movielens-06 \\\n",
    "    -file ./codes/avgGenreMapper02.py \\\n",
    "    -mapper avgGenreMapper02.py \\\n",
    "    -file ./codes/avgGenreReducer02.py \\\n",
    "    -reducer avgGenreReducer02.py \\\n",
    "    -file ./codes/avgGenreCombiner.py \\\n",
    "    -combiner avgGenreCombiner.py \\\n",
    "    -file ./movielens/movies.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How different are the number of shuffle bytes between the two jobs?**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 (Anaconda)",
   "language": "python",
   "name": "anaconda_py2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
