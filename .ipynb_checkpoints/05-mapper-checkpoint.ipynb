{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "layout: page\n",
    "title: Introduction to Hadoop\n",
    "subtitle: Creating a Mapper\n",
    "minutes: 20\n",
    "---\n",
    "> ## Learning Objectives {.objectives}\n",
    ">\n",
    "> *   Understand the process of viewing data in terms of Key/Value pair\n",
    "> *   Understand the process of going backward from the desired answer to\n",
    ">     the logic for reducer and then mapper\n",
    "> *   Create python-based mapper function and test against non-Hadoop input\n",
    "\n",
    "Now that we are familiar with Hadoop basic commands, it is time to revisit the\n",
    "initial analyses on the movie dataset. Let's start with one simple analysis,\n",
    "which is to find rating averages of movies over the years.\n",
    "\n",
    "We first examine the structure of data file via http://files.grouplens.org/datasets/movielens/ml-10m-README.html\n",
    "The output of this file should be visible in Jupyter's output box. You can\n",
    "toggle scrolling of this output instead of having a lengthy window by clicking\n",
    "on Cells/Current Outputs/Toggle Scrolling on the Menu bar. \n",
    "\n",
    "**User Ids**\n",
    "\n",
    "Movielens users were selected at random for inclusion. Their ids have been anonymized. Users were selected separately \n",
    "for inclusion in the ratings and tags data sets, which implies that user ids may appear in one set but not the other. \n",
    "The anonymized values are consistent between the ratings and tags data files. That is, user id n, if it appears in \n",
    "both files, refers to the same real MovieLens user. \n",
    "\n",
    "**Ratings Data File Structure** \n",
    "\n",
    "All ratings are contained in the file ratings.dat. Each line of this file represents one rating of one movie by one user, and has the following format: \n",
    "\n",
    "UserID::MovieID::Rating::Timestamp \n",
    "\n",
    "The lines within this file are ordered first by UserID, then, within user, by MovieID. Ratings are made on a 5-star scale, with half-star increments. \n",
    "\n",
    "Timestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970. \n",
    "\n",
    "**Tags Data File Structure** \n",
    "\n",
    "All tags are contained in the file tags.dat. Each line of this file represents one tag applied to one movie by one\n",
    "user, and has the following format: \n",
    "\n",
    "UserID::MovieID::Tag::Timestamp \n",
    "\n",
    "The lines within this file are ordered first by UserID, then, within user, by MovieID. Tags are user generated \n",
    "metadata about movies. Each tag is typically a single word, or short phrase. The meaning, value and purpose of a \n",
    "particular tag is determined by each user. \n",
    "\n",
    "Timestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970. \n",
    "\n",
    "**Movies Data File Structure** \n",
    "\n",
    "Movie information is contained in the file movies.dat. Each line of this file represents one movie, and has the following format: \n",
    "\n",
    "MovieID::Title::Genres \n",
    "\n",
    "MovieID is the real MovieLens id. \n",
    "\n",
    "Movie titles, by policy, should be entered identically to those found in IMDB, including year of release. \n",
    "However, they are entered manually, so errors and inconsistencies may exist. Genres are a pipe-separated list, \n",
    "and are selected from the following: \n",
    "\n",
    "-   Action\n",
    "-   Adventure\n",
    "-   Animation\n",
    "-   Children's\n",
    "-   Comedy\n",
    "-   Crime\n",
    "-   Documentary\n",
    "-   Drama\n",
    "-   Fantasy\n",
    "-   Film-Noir\n",
    "-   Horror\n",
    "-   Musical\n",
    "-   Mystery\n",
    "-   Romance\n",
    "-   Sci-Fi\n",
    "-   Thriller\n",
    "-   War\n",
    "-   Western\n",
    "\n",
    "To find the rating averages of movies, we will need to look at the\n",
    "**ratings.csv** file. In this case, the dataset's README has provided extensive\n",
    "details about the structure of the file. In the case when a README is not\n",
    "available or inadequate, you can attempt to discern this information by looking\n",
    "at the data itself. It is important to note that you do not want to view the\n",
    "entire file (if it is stored in Hadoop, it is going to be HUGE). Since HDFS\n",
    "does not have a **head** function, you can call **-cat** but pipe the results\n",
    "through the Linux system's **head** function to cut off the output stream at\n",
    "the number of lines you want to review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1::122::5::838985046\r\n",
      "1::185::5::838983525\r\n",
      "1::231::5::838983392\r\n",
      "1::292::5::838983421\r\n",
      "1::316::5::838983392\r\n",
      "1::329::5::838983392\r\n",
      "1::355::5::838984474\r\n",
      "1::356::5::838983653\r\n",
      "1::362::5::838984885\r\n",
      "1::364::5::838983707\r\n"
     ]
    }
   ],
   "source": [
    "!ssh dsciu001 hdfs dfs -cat /user/lngo/intro-to-hadoop/ml-10M100K/ratings.dat \\\n",
    "    2>/dev/null | head -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For any specific job/task that requires MapReduce, the inevitable question is\n",
    "what is the Key/Value pair the mapper should produce such that the collection\n",
    "of Values for each unique Key at the reducer is suitable for the operation that\n",
    "would generate the final required results. One approach to address this question\n",
    "is to traverse backward from the required results. In our case, we want to find\n",
    "the rating average of each movie within the dataset. It is intuitive then to\n",
    "assume that a movie should be the KEY, and the collection of VALUES is the\n",
    "collection of ratings across the entire data set for this movie. Therefore, the\n",
    "KEY/VALUE pair emitted by the mapping process should be movie/rating. Based on\n",
    "the structure of the **ratings.csv** file, this is equivalent to emitting the\n",
    "elements at the second and third columns as KEY and VALUE, respectively.  \n",
    "\n",
    "Under Jupyter's main page, select New and open new Terminal. This will pop up an\n",
    "additional browser tab with a terminal to your allocated Palmetto node. Within\n",
    "this terminal, navigate to the **intro-to-hadoop** directory and create a Python\n",
    "file named mapper01.py with the following content:\n",
    "\n",
    "~~~ {.bash}\n",
    "#!/usr/bin/env python                                          \n",
    "import sys                                                                                                \n",
    "for oneMovie in sys.stdin:                                                                            \n",
    "  oneMovie = oneMovie.strip()                                                  \n",
    "  ratingInfo = oneMovie.split(\"::\")                                                                       \n",
    "  movieID = ratingInfo[1]                                                                               \n",
    "  rating = ratingInfo[2]                                                                                 \n",
    "  print ('%s\\t%s' % (movieID, rating))           \n",
    "~~~\n",
    "\n",
    "We can test this mapper file by using Linux pipe and HDFS' **cat** command in\n",
    "the following manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122\t5\r\n",
      "185\t5\r\n",
      "231\t5\r\n",
      "292\t5\r\n",
      "316\t5\r\n",
      "329\t5\r\n",
      "355\t5\r\n",
      "356\t5\r\n",
      "362\t5\r\n",
      "364\t5\r\n"
     ]
    }
   ],
   "source": [
    "!ssh dsciu001 hdfs dfs -cat /user/lngo/intro-to-hadoop/ml-10M100K/ratings.dat \\\n",
    "    2>/dev/null | head -n 10 | python /home/lngo/intro-to-hadoop/mapper01.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The mapping output looks good, but it does not carry as much information as we\n",
    "would like. As it is, this intermediate data will enable reducers to calculate\n",
    "the average ratings associated with movieId. While this is technically correct,\n",
    "it still requires additional processing to associate the IDs with the movie\n",
    "titles themselves. Is it possible for us to do this step within the mapper?\n",
    "Reexamining the README, we see that movieIds are associated with titles in the\n",
    "**movies.dat** file. Also, this file is much smaller than the **ratings.dat**\n",
    "file. Looking forward into the future, we can see that this difference in size\n",
    "can only be widened, as there can only be hundreds to thousands of new movies\n",
    "each year, but there will be significantly more people reviewing each movie\n",
    "each year. Therefore, it is reasonable to have each **individual mapper** to go\n",
    "ahead and load **movies.dat** as dictionary in order to associate each processed\n",
    "movieId with the appropriate title.\n",
    "\n",
    "It is easier to have **movies.dat** available to be loaded from the Linux\n",
    "file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapper01.py  mapper03.py  ml-10m.zip  movies.dat\r\n",
      "mapper02.py  ml-10M100K   movies.csv  reducer01.py\r\n"
     ]
    }
   ],
   "source": [
    "!ssh dsciu001 hdfs dfs -get /user/lngo/intro-to-hadoop/ml-10M100K/movies.dat /home/lngo/intro-to-hadoop 2>/dev/null\n",
    "!ls /home/lngo/intro-to-hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The easiest way to streamline the association process is to read the\n",
    "**movies.csv** file as a Python dictionary. Create a Python file named\n",
    "**mapper02.py** with the following content:\n",
    "\n",
    "~~~ {.bash}\n",
    "#!/usr/bin/env python   \n",
    "import os\n",
    "import sys            \n",
    "import csv                                                       \n",
    "movieFile = os.path.join(os.path.dirname(sys.argv[0]),\"movies.dat\")\n",
    "movieList = {}\n",
    "with open(movieFile, mode = 'r') as infile:\n",
    "  for line in infile:\n",
    "    row = (line.strip()).split(\"::\")\n",
    "    movieList[row[0]] = {}  \n",
    "    movieList[row[0]][\"title\"] = row[1].strip()  \n",
    "    movieList[row[0]][\"genre\"] = row[2].strip()                       \n",
    "for oneMovie in sys.stdin:                                     \n",
    "  oneMovie = oneMovie.strip()      \n",
    "  ratingInfo = oneMovie.split(\"::\")               \n",
    "  try:\n",
    "    movieTitle = movieList[ratingInfo[1]][\"title\"]   \n",
    "    rating = float(ratingInfo[2])                     \n",
    "    print (\"%s\\t%s\" % (movieTitle, rating))    \n",
    "  except ValueError:                           \n",
    "    continue               \n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boomerang (1992)\t5.0\r\n",
      "Net, The (1995)\t5.0\r\n",
      "Dumb & Dumber (1994)\t5.0\r\n",
      "Outbreak (1995)\t5.0\r\n",
      "Stargate (1994)\t5.0\r\n",
      "Star Trek: Generations (1994)\t5.0\r\n",
      "Flintstones, The (1994)\t5.0\r\n",
      "Forrest Gump (1994)\t5.0\r\n",
      "Jungle Book, The (1994)\t5.0\r\n",
      "Lion King, The (1994)\t5.0\r\n"
     ]
    }
   ],
   "source": [
    "!ssh dsciu001 hdfs dfs -cat /user/lngo/intro-to-hadoop/ml-10M100K/ratings.dat 2>/dev/null \\\n",
    "    | head -n 10 | python /home/lngo/intro-to-hadoop/mapper02.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It should be noted that the variable ***movieFile*** is opened using a path relative to the **mapper02.py** file. This is an important detail that will be necessary for running the MapReduce process on an actual Hadoop cluster. \n",
    "\n",
    "\n",
    "> ## Data Checking {.callout}\n",
    ">\n",
    "> Notice the .strip() statement. Leading and trailing white spaces are among the\n",
    "> most common potential errors, especially when dealing with textual data.\n",
    "\n",
    "We are now ready to funnel this data into a reducer.\n",
    "\n",
    "## Check your understanding: Map genres to ratings {.challenge}\n",
    "Another useful information to the movie company is not about the average ratings\n",
    "of movies, but the average ratings of the genres. Write a mapper program called\n",
    "**mapGenre.py** to associate the rating information of the movies to their\n",
    "respective genres. A movie can belong to several genres, and its rating will be\n",
    "counted as the rating for each of its genres."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda 2.5.0 (Python 3)",
   "language": "python",
   "name": "anaconda_2.5.0_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
