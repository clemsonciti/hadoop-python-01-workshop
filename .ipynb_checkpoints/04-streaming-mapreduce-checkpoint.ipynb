{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "layout: page\n",
    "title: Introduction to Hadoop\n",
    "subtitle: Streaming MapReduce\n",
    "minutes: 10\n",
    "---\n",
    "> ## Learning Objectives {.objectives}\n",
    ">\n",
    "> *   Understand how to run Hadoop MapReduce programs\n",
    "> *   Create Hadoop MapReduce commands to run external programs as\n",
    ">     mappers and reducers\n",
    "\n",
    "Any Application-related interactions between users, the Hadoop MapReduce\n",
    "framework, and HDFS are done though YARN, Hadoop' default resource manager. The\n",
    "most common interactions include submitting applications to YARN and inquiring\n",
    "about the status of the applications. The generic syntax is:\n",
    "\n",
    "    yarn COMMAND --loglevel loglevel [generic_options] [command_options]\n",
    "\n",
    "Starting from the least to the most verbose, we have FATAL, ERROR, WARN, INFO,\n",
    "DEBUG, and TRACE. Default level is INFO.\n",
    "\n",
    "generic_options | Description |\n",
    "----------------|-------------|\n",
    "`-archives <comma separated list of archives>` | Specify archives to be unarchived on the compute machines. Applies only to job |\n",
    "`-conf <configuration file>` | Specify an application configuration file |\n",
    "`-D <property>=<value>` | Use value for a given property |\n",
    "`-files <comma separated list of files>` | Specify files to be copied to the cluster. Applies only to job |\n",
    "`-jt <local> or <resourcemanager:port>` | Specify a ResourceManager. Applies only to job |\n",
    "`-libjars <comma separated list of jars>` | Specify the jar files to include in the classpath. Applies only to job.\n",
    "\n",
    "#### COMMAND: jar\n",
    "Run a jar file as an application on Cypress. Usage:\n",
    "\n",
    "    yarn jar <jar file> [mainClass] args ...\n",
    "\n",
    "Typically, YARN applications are written in Java and bundled into jar files to be executed. However, Hadoop also supports the execution of non-Java applications via the Hadoop Streaming utility. This utility allows you to use any executable or scripts as the mapper and/or the reducer for a YARN application. Usage:\n",
    "\n",
    "    yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar [generic_options] [streaming_options]\n",
    "\n",
    "streaming_options | Optional/Required | Description |\n",
    "------------------|-------------------|-------------|\n",
    "`-input directoryname or filename` | Required | Input location for mapper |\n",
    "`-output directoryname` | Required | Output location for reducer |\n",
    "\n",
    "`-mapper executable or JavaClassName` | Required | Mapper executable |\n",
    "`-reducer executable or JavaClassName` | Required | Reducer executable |\n",
    "`-file filename` | Optional |  Make the mapper, reducer, or combiner executable available locally on the compute nodes |\n",
    "`-inputformat JavaClassName` | Optional | Class you supply should return key/value pairs of Text class. If not specified, TextInputFormat is used as the default |\n",
    "`-outputformat JavaClassName` | Optional | Class you supply should take key/value pairs of Text class. If not specified, TextOutputformat is used as the default |\n",
    "`-partitioner JavaClassName` | Optional | Class that determines which reduce a key is sent to |\n",
    "`-combiner executable or JavaClassName` | Optional | Combiner executable for map output |\n",
    "`-cmdenv name=value` | Optional | Pass environment variable to streaming commands |\n",
    "`-inputreader JavaClassName` | Optional | For backwards-compatibility: specifies a record reader class (instead of an input format class) |\n",
    "`-verbose` | Optional | Verbose output |\n",
    "`-lazyOutput` | Optional | Create output lazily. For example, if the output format is based on FileOutputFormat, the output file is created only on the first call to Context.write |\n",
    "`-numReduceTasks` | Optional | Specify the number of reducers |\n",
    "`-mapdebug` | Optional | Script to call when map task fails |\n",
    "`-reducedebug` | Optional | Script to call when reduce task fails |\n",
    "\n",
    "To demonstrate Hadoop Streaming functionality, let's look at the problem of\n",
    "counting how many instances of \"thou\" there are in Gutenberg's complete work of\n",
    "Shakespeare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/hdp/2.4.2.0-258/hadoop-mapreduce/hadoop-streaming-2.7.1.2.4.2.0-258.jar] /var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir/streamjob3414726412006351394.jar tmpDir=null\n",
      "16/07/25 13:01:43 INFO impl.TimelineClientImpl: Timeline service address: http://dscim003.palmetto.clemson.edu:8188/ws/v1/timeline/\n",
      "16/07/25 13:01:44 INFO impl.TimelineClientImpl: Timeline service address: http://dscim003.palmetto.clemson.edu:8188/ws/v1/timeline/\n",
      "16/07/25 13:01:44 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 857 for lngo on ha-hdfs:dsci\n",
      "16/07/25 13:01:44 INFO security.TokenCache: Got dt for hdfs://dsci; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:dsci, Ident: (HDFS_DELEGATION_TOKEN token 857 for lngo)\n",
      "16/07/25 13:01:44 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/07/25 13:01:45 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/07/25 13:01:45 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1469462752582_0008\n",
      "16/07/25 13:01:45 INFO mapreduce.JobSubmitter: Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:dsci, Ident: (HDFS_DELEGATION_TOKEN token 857 for lngo)\n",
      "16/07/25 13:01:45 INFO impl.YarnClientImpl: Submitted application application_1469462752582_0008\n",
      "16/07/25 13:01:45 INFO mapreduce.Job: The url to track the job: http://dscim001.palmetto.clemson.edu:8088/proxy/application_1469462752582_0008/\n",
      "16/07/25 13:01:45 INFO mapreduce.Job: Running job: job_1469462752582_0008\n",
      "16/07/25 13:02:01 INFO mapreduce.Job: Job job_1469462752582_0008 running in uber mode : false\n",
      "16/07/25 13:02:01 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/07/25 13:02:18 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/07/25 13:02:20 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/07/25 13:02:31 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/07/25 13:02:31 INFO mapreduce.Job: Job job_1469462752582_0008 completed successfully\n",
      "16/07/25 13:02:31 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5820389\n",
      "\t\tFILE: Number of bytes written=12081444\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=5476614\n",
      "\t\tHDFS: Number of bytes written=25\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=30741\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=22468\n",
      "\t\tTotal time spent by all map tasks (ms)=30741\n",
      "\t\tTotal time spent by all reduce tasks (ms)=11234\n",
      "\t\tTotal vcore-seconds taken by all map tasks=30741\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=11234\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=251830272\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=184057856\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=124213\n",
      "\t\tMap output records=124213\n",
      "\t\tMap output bytes=5571957\n",
      "\t\tMap output materialized bytes=5820395\n",
      "\t\tInput split bytes=230\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=111202\n",
      "\t\tReduce shuffle bytes=5820395\n",
      "\t\tReduce input records=124213\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=248426\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=728\n",
      "\t\tCPU time spent (ms)=22740\n",
      "\t\tPhysical memory (bytes) snapshot=2323980288\n",
      "\t\tVirtual memory (bytes) snapshot=34513907712\n",
      "\t\tTotal committed heap usage (bytes)=2091384832\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=5476384\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=25\n",
      "16/07/25 13:02:31 INFO streaming.StreamJob: Output directory: intro-to-hadoop/wc\n"
     ]
    }
   ],
   "source": [
    "!ssh dsciu001 yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input intro-to-hadoop/gutenberg-shakespeare.txt  \\\n",
    "    -output intro-to-hadoop/wc -mapper cat -reducer \"wc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512m; support was removed in 8.0\n",
      "Found 2 items\n",
      "-rw-r--r--   2 lngo hdfs          0 2016-07-25 13:02 intro-to-hadoop/wc/_SUCCESS\n",
      "-rw-r--r--   2 lngo hdfs         25 2016-07-25 13:02 intro-to-hadoop/wc/part-00000\n"
     ]
    }
   ],
   "source": [
    "!ssh dsciu001 hdfs dfs -ls intro-to-hadoop/wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512m; support was removed in 8.0\n",
      " 124213  899681 5571957\t\n"
     ]
    }
   ],
   "source": [
    "!ssh dsciu001 hdfs dfs -cat intro-to-hadoop/wc/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Several items need to be paid attention to in the above example. First, the\n",
    "location of the input and output directories are relative. That is, without an\n",
    "initial backslash (**/**), YARN assumes that the path starts from inside user's\n",
    "home directory on HDFS, which is **/user/user-name**. Second, the output\n",
    "directory must not exist prior to the *yarn jar* call, otherwise, the command\n",
    "will fail with an *output directory already exists* error.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda 2.5.0 (Python 3)",
   "language": "python",
   "name": "anaconda_2.5.0_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
