{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "layout: page\n",
    "title: Introduction to Hadoop\n",
    "subtitle: Introducing the MapReduce Programming Paradigm\n",
    "minutes: 20\n",
    "---\n",
    "> ## Learning Objectives {.objectives}\n",
    ">\n",
    "> *   Understand the MapReduce programming paradigm\n",
    "> *   Understand why the MapReduce programming paradigm is a natural approach\n",
    ">     to handle HDFS data with the Hadoop MapReduce implementation\n",
    "> *   Understand Key/Value pair    \n",
    "\n",
    "MapReduce is a programming model that has its roots in functional programming.\n",
    "The ideal targets for MapReduce are collections of data elements (lists, arrays,\n",
    "sets ...). There are two core functions in MapReduce: Map and Reduce.\n",
    "\n",
    "Map operates on all data elements of a collection by applying the same operation\n",
    "(or same set of operations) to each individual element of this collection. The\n",
    "outcome of Map is another collection of new data elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 4, 9, 16]\n"
     ]
    }
   ],
   "source": [
    "input_data = [0,1,2,3,4]\n",
    "\n",
    "def squared (x):\n",
    "    return x * x\n",
    "\n",
    "map_output_1 = list(map(squared, input_data))\n",
    "print (map_output_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map operation to filter data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, None, 9, 16]\n"
     ]
    }
   ],
   "source": [
    "input_data = [0,1,2,3,4]\n",
    "\n",
    "def squared (x):\n",
    "    tmp = x * x\n",
    "    if tmp > 4:\n",
    "        return x * x\n",
    "\n",
    "map_output_2 = list(map(squared, input_data))\n",
    "print (map_output_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map operation to break up sentences into individual words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Ask', 'not', 'what', 'your', 'data', 'can', 'do', 'for', 'you'], ['ask', 'what', 'you', 'can', 'do', 'for', 'your', 'data']]\n"
     ]
    }
   ],
   "source": [
    "input_data = [\"Ask not what your data can do for you\", \"ask what you can do for your data\"]\n",
    "\n",
    "def parse_words (x):\n",
    "    return x.split()\n",
    "\n",
    "map_output_3 = list(map(parse_words, input_data))\n",
    "print (map_output_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Reduce function will operate on the outcome of a Map operation to either\n",
    "collapse or combine these new data elements into either a single value or a\n",
    "subset of elements.\n",
    "\n",
    "Reduce function that accumulates values of a list using Python's built-in\n",
    "***reduce*** function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "def sum(tmp, x):\n",
    "    return tmp + x\n",
    "\n",
    "reduce_output_1 = functools.reduce(sum, map_output_1)\n",
    "print (reduce_output_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A user-defined reduce function using standard for loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "def sum_reduce(x):\n",
    "    sum = 0\n",
    "    for data_element in x:\n",
    "        sum = sum + data_element\n",
    "    return sum\n",
    "\n",
    "reduce_output_2 = sum_reduce(map_output_1)\n",
    "print(reduce_output_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Check your understanding: Write a MapReduce procedure {.challenge}\n",
    "\n",
    "Given the following data set [-2,5,-10,4,7,9,1], find the largest squared value.\n",
    "\n",
    "## Hadoop MapReduce\n",
    "\n",
    "We recall from lessons [1](00-intro.html) and [3](02-filedir.html) that HDFS\n",
    "divides big data files into small blocks and distributes these blocks across a\n",
    "network of computers. In order to support the ***data locality*** concept, we\n",
    "need to bring the required computation to these data blocks. The MapReduce\n",
    "programming paradigm lends itself naturally to this concept.\n",
    "\n",
    "The Map operation can be thought of as having the same operation being applied\n",
    "to each data elements in a collection. Therefore, in HDFS setting, the same\n",
    "Map operation can be applied to individual data blocks of a file. As these\n",
    "blocks are distributed across computers, the processors on these computers can\n",
    "execute the operations in parallel, significantly improving performance.\n",
    "\n",
    "After the Map operation is completed, since the blocks are located on different\n",
    "computers, the output data of the Map operation is naturally also distributed\n",
    "across these computers. For the Reduce operation, a number of issues must be\n",
    "addressed, such as:  \n",
    "\n",
    "1. How can we gather the map output data for reduction?\n",
    "2. How can we also speed up the Reduce process?\n",
    "\n",
    "Hadoop MapReduce uses several mechanisms to resolve these issues.\n",
    "\n",
    "**Key/Value Pair**: For Hadoop MapReduce, data are represented not as a single\n",
    "data value, but as a tuple of Key and Value. The key could be a unique\n",
    "identifier or a representative attribute of the data value. The key enables\n",
    "the Hadoop MapReduce framework to group data values of the same type or\n",
    "characteristics together.\n",
    "\n",
    "**Shuffle**: Hadoop MapReduce will ***shuffles*** map output data across\n",
    "computers to group data with the same key into collections. The Reduction\n",
    "operation will be applied to these collections. As the collections will be\n",
    "distributed, the Reduce process also happens in parallel.\n",
    "\n",
    "**Partition**: Hadoop MapReduce will ***partition*** the placement of these\n",
    "collections such that they are balanced across the computers and minimal data\n",
    "transfer is needed.\n",
    "\n",
    "Hadooop MapReduce carries default implementations of ***Shuffle*** and\n",
    "***Partition*** functions. Together with the management of data distribution\n",
    "via HDFS, that leaves users with only the task of developing the Map and the\n",
    "Reduce operation, in which determining Key and Value is a critical step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda 2.5.0 (Python 3)",
   "language": "python",
   "name": "anaconda_2.5.0_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
