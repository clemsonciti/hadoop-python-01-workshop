{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Introduction to Hadoop MapReduce for Big Data </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <center> Cyberinfrastructure and Technology Integration\n",
    "\n",
    "http://citi.sites.clemson.edu\n",
    "\n",
    "https://www.palmetto.clemson.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <center> A quick history of Hadoop:\n",
    "\n",
    "-   2002: Doug Cutting and Mike Carafella started a project to build an open-source search engine called Nutch. \n",
    "    A component of this project was a web crawler that can crawl and index the Internet.\n",
    "-   2003: Google released a research paper on its in-house data storage system \n",
    "    called [Google File System](http://dl.acm.org/citation.cfm?id=945450) (GFS).\n",
    "-   2004: Google released another research paper on the programming approach to process data stored on GFS, \n",
    "    called [MapReduce](http://dl.acm.org/citation.cfm?id=1327492).\n",
    "-   2005: Cutting and Carafelle rebuilt the underlying file management system and processing framework of Nutch \n",
    "    based on the architectural design of Google's GFS and MapReduce.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "-   2006: The adaptations of Google's GFS and MapReduce are converted into a single open source project called \n",
    "    Hadoop, which is sponsored by Yahoo and led by Doug Cutting.\n",
    "-   2007: Yahoo maintains a 1000-node production cluster.\n",
    "-   2008: Hadoop becomes the platform of Yahoo's web index. Hadoop wins record for world fastest \n",
    "    system to sort one terabyte of data (209 seconds using a 910-node cluster). Hadoop becomes a \n",
    "    top-level open source project of Apache Foundation. First Hadoop commercial distributor led \n",
    "    by a former Google employee, Cloudera, is founded.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "-   2009: Hadoop sorts one terabyte of data in 62 seconds and one petabyte of data in 16.25 hours. Second Hadoop \n",
    "    commercial distributor, MapR, is formed.\n",
    "-   2011: Yahoo spins off its own Hadoop comnmercial distributor, Hortonworks.\n",
    "-   2012: Apache Hadoop 1.0 is released."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <center> What Makes HDFS Different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../fig/StorageSimplified.png\" \\\n",
    "     alt=\"Simple Presentation of Storage Models\" \\\n",
    "     style=\"height:500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "There are three approaches in storing data for processing purposes:\n",
    "\n",
    "-   Data are stored on a single computer's local hard drive and can be processed by programs running on \n",
    "    the same computer. This is how most people manage their data in normal daily tasks.\n",
    "-   Data are stored on remote storage systems. These systems are often consisted of multiple hard drives \n",
    "    to support reading/writing large amount of data. Software programs accessing these data are located \n",
    "    on a different set of computers, and the data must be copied from the storage systems to these computers \n",
    "    over the network. This is the storage model of the Palmetto Supercomputer.\n",
    "-   Data are stored on a set of computers, and the software programs accessing these data also runs on \n",
    "    the same set of computers. This is the storage model of the Hadoop Distributed File System."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <center> HDFS Design Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "-   Hardware failure is the norm rather than the exception. \n",
    "-   Data arrives as a stream and to be processed as sequential batches (no random access).\n",
    "-   The amount of data to be processed is very large.\n",
    "-   Data are written once but read many times (no data modification).\n",
    "-   It is cheaper to move the computation (e.g., copy the programs) than to move the data.\n",
    "-   The set of computers contains different types of hardware and software."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <center> Movie Ratings and Recommendation </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An independent movie company is looking to invest in a new movie project. With limited finance, the company wants to \n",
    "analyze the reaction of audiences, particularly toward various movie genres, in order to identify beneficial \n",
    "movie project to focus on. The company relies on data collected from a publicly available recommendation service \n",
    "by [MovieLens](http://dl.acm.org/citation.cfm?id=2827872). This \n",
    "[dataset](http://files.grouplens.org/datasets/movielens/ml-10m-README.html) \n",
    "contains \"**10,000,054** ratings and **95,580** tag applications across **10,681** movies. These data were created \n",
    "by **71,567** users between January 09, 1995 and January 29, 2016.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "From this dataset, several analyses are possible, include the followings:\n",
    "\n",
    "1.   Find movies which have the highest ratings over the years and identify the corresponding genre.\n",
    "2.   Find genres which have the highest ratings over the years.\n",
    "3.   Find users who rate movies most frequently in order to contact them for in-depth marketing analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "These types of analyses, which are somewhat ambiguous, demand the ability to quickly process large amount of data in \n",
    "elatively short amount of time for decision support purposes. In these situations, the sizes of the data typically \n",
    "make analysis done on a single machine impossible and analysis done using a remote storage system impractical. For \n",
    "remainder of the lessons, we will learn how HDFS provides the basis to store massive amount of data and to enable \n",
    "the programming approach to analyze these data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this workshop, we will leverage the Jupyter infrastructure at Clemson\n",
    "University to directly interact with Hadoop. Technical details on how to use \n",
    "the Jupyter infrastructure can be found at **https://clemsonciti.github.io/jupyterhub-userdocs/**\n",
    "\n",
    "For this workshop, the default codes inside a cell will be interpreted as Python\n",
    " language. However, any line that begins with **!** will be interpreted as a\n",
    " Linux system command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <center> Challenge 1 </center>\n",
    "\n",
    "- View the content of your HDFS user directory (/user/**your-username**) on  Cypress\n",
    "- Check your understanding: Create directory on HDFS\n",
    "- Create a directory in your HDFS user directory named **intro-to-hadoop**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <center> Challenge 2 </center>\n",
    "\n",
    "- Copy the file ***gutenberg-shakespeare.txt*** from Palmetto to this newly created **intro-to-hadoop** directory on HDFS using **put**. View the content of  the **intro-to-hadoop** directory to confirm that the file has been  successfully uploaded.\n",
    " \n",
    "- Use `wget` to download the movie rating data from https://github.com/clemsonciti/hadoop-python-01-workshop/raw/gh-pages/data/ml-10m.zip. Use `unzip` to decompress the file. Copy the decompressed directory, **ml-10M100K**, into the **intro-to-hadoop** directory on HDFS using **put**. Run `hdfs fsck` on this directory to validate the status of the uploaded directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <center> HDFS Files and Directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "More than just a file storage and management system, HDFS provides an\n",
    "infrastructure through which parallel processing of massive amount of data is\n",
    "enabled.\n",
    "\n",
    "<img src=\"../fig/HDFSBlockView.png\" \\\n",
    "     alt=\"HDFSBlockView\" \\\n",
    "     style=\"height:500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To enable large scale processing of big data, Hadoop takes a straight forward\n",
    "approach in HDFS, which is to simply divide a very large data file into\n",
    "smaller blocks and distribute these blocks across a cluster of computers\n",
    "(the Hadoop cluster). The blocks are replicated to ensure that if any\n",
    "individual computer fails, there are still enough copies of the data on the\n",
    "remaining computers for uninterrupted operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To bring out the nature of data locality in this distributed block-based\n",
    "approach, it is critical to minimize the needs for data transfer between\n",
    "computers storing these data blocks. A programming approach called\n",
    "***mapreduce*** is leveraged by Google to make this happen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <center> mapreduce vs Apache MapReduce </center>\n",
    "\n",
    "It is important to distinguish between the mapreduce programming paradigm and the Apache MapReduce implementation. \n",
    "\n",
    "- The mapreduce programming paradigm includes any implementation approach that ***maps*** the same operation to individual data elements of a data collection, and then ***reduce*** the resulting data to a final simplified result. For example, Apache Spark, the highly touted \"MapReduce killer\", utilizes in-memory operations to implement its mapping and reducing capabilities. \n",
    "\n",
    "- Apache MapReduce is the default implementation of the mapreduce paradigm for Hadoop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "MapReduce is a programming model that has its roots in functional programming.\n",
    "The ideal targets for MapReduce are collections of data elements (lists, arrays,\n",
    "sets ...). There are two core functions in MapReduce: Map and Reduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Map operates on all data elements of a collection by applying the same operation (or same set of operations) to each individual element of this collection. The outcome of Map is another collection of new data elements "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A Reduce function will operate on the outcome of a Map operation to either collapse or combine these new data elements into either a single value or a subset of elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <center> Hadoop MapReduce </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- HDFS divides big data files into small blocks and distributes these blocks across a network of computers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In order to support the ***data locality*** concept, we need to bring the required computation to these data blocks. The MapReduce programming paradigm lends itself naturally to this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The Map operation can be thought of as having the same operation being applied to each data elements in a collection. Therefore, in HDFS setting, the same Map operation can be applied to individual data blocks of a file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As these blocks are distributed across computers, the processors on these computers can execute the operations in parallel, significantly improving performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "After the Map operation is completed, since the blocks are located on different\n",
    "computers, the output data of the Map operation is naturally also distributed\n",
    "across these computers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. How can we gather the map output data for reduction?\n",
    "2. How can we also speed up the Reduce process?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hadoop MapReduce uses several mechanisms to resolve these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Key/Value Pair**: For Hadoop MapReduce, data are represented not as a single\n",
    "data value, but as a tuple of Key and Value. The key could be a unique\n",
    "identifier or a representative attribute of the data value. The key enables\n",
    "the Hadoop MapReduce framework to group data values of the same type or\n",
    "characteristics together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Shuffle**: Hadoop MapReduce will ***shuffles*** map output data across\n",
    "computers to group data with the same key into collections. The Reduction\n",
    "operation will be applied to these collections. As the collections will be\n",
    "distributed, the Reduce process also happens in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Partition**: Hadoop MapReduce will ***partition*** the placement of these\n",
    "collections such that they are balanced across the computers and minimal data\n",
    "transfer is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hadooop MapReduce carries default implementations of ***Shuffle*** and\n",
    "***Partition*** functions. Together with the management of data distribution\n",
    "via HDFS, that leaves users with only the task of developing the Map and the\n",
    "Reduce operation, in which determining Key and Value is a critical step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>**What is the average rating of each movie over the years**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <center> Challenge 3  </center>\n",
    "\n",
    "- Write a mapper program called **mapGenre.py** to associate the rating information of the movies to their respective genres. A movie can belong to several genres, and its rating will be counted as the rating for each of its genres.\n",
    "\n",
    "- While the reducer code uses variables such as movie and current_movie, in principle, this reducer can take any set of KEY/GROUP OF VALUES and calculate the average value of this KEY. Use this reducer together with mapGenre.py to test the finding of rating averages of movie genres."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Anaconda 2.5.0 (Python 3)",
   "language": "python",
   "name": "anaconda_2.5.0_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
