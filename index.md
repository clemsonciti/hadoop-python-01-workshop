---
layout: page
title: Introduction to Hadoop
---

> ## Learning objectives {.objectives}
>
> By the end of this workshop, you will be able to
>
> 1. Create, manage and navigate files and directories on
>    the Hadoop Distributed File System (HDFS).
> 2. Understand the concept of data locality in distributed system.
> 3. Understand how big data files are distributed across HDFS and manipulated by MapReduce programming paradigm to facilitate data locality.
> 3. Write MapReduce Python programs to analyze a large data set.
> 4. Integrate MapReduce processing with standard Python programs to facilitate complex analysis.

> ## Prerequesites {.prereq}
> This workshop requires prerequisites knowledges that are equivalent to the following COE workshops:
> 1. Introduction to research computing on the Palmetto Cluster
> 2. Introduction to Linux
> 3. Introduction to Python

## Contributing

These lessons are modeled after the structure of
[Data Carpentry][dc-lessons] lesson materials,
an open source project.
Like Data Carpentry, we welcome contributions
of all kinds:
new lessons,
fixes/improvements to existing material,
corrections to typos,
bug reports,
and reviews of proposed changes are all equally welcome.
Please see our page on [Contributing][contributing]
to get started.

## Introduction to Hadoop

1. [Introduction to the Unix shell](00-intro.html)
2. [Accessing the Palmetto cluster](01-accessing-palmetto.html)
3. [Files and directories](02-filedir.html)
4. [Creating things](03-create.html)
5. [Pipes and filters](04-pipefilter.html)
6. [Performing repetitive tasks](05-loop.html)
7. [Shell scripts](06-shell-scripts.html)
8. [Finding things](07-find.html)
9. [File permissions](08-permissions.html)

[swc-lessons]: https://software-carpentry.org/lessons/
[contributing]: https://github.com/shwina/hpc-novice/blob/gh-pages/CONTRIBUTING.md
