{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "layout: page\n",
    "title: Introduction to Hadoop\n",
    "subtitle: Integrating Python Mapper and Reducer in Hadoop\n",
    "minutes: 15\n",
    "---\n",
    "> ## Learning Objectives {.objectives}\n",
    ">\n",
    "> *   Run the combination of Python-based mapper and reducer on the Hadoop\n",
    ">     infrastructure\n",
    "> *   Customize reducer for questions that require global access to KEYS\n",
    "\n",
    "With the mapper and reducer created and tested, the final step is to run this\n",
    "combination on the Hadoop infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/07/25 15:31:35 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/home/lngo/intro-to-hadoop/mapper02.py, /home/lngo/intro-to-hadoop/reducer01.py, /home/lngo/intro-to-hadoop/movies.dat] [/usr/hdp/2.4.2.0-258/hadoop-mapreduce/hadoop-streaming-2.7.1.2.4.2.0-258.jar] /var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir/streamjob8030563500250963142.jar tmpDir=null\n",
      "16/07/25 15:31:37 INFO impl.TimelineClientImpl: Timeline service address: http://dscim003.palmetto.clemson.edu:8188/ws/v1/timeline/\n",
      "16/07/25 15:31:37 INFO impl.TimelineClientImpl: Timeline service address: http://dscim003.palmetto.clemson.edu:8188/ws/v1/timeline/\n",
      "16/07/25 15:31:37 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 860 for lngo on ha-hdfs:dsci\n",
      "16/07/25 15:31:37 INFO security.TokenCache: Got dt for hdfs://dsci; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:dsci, Ident: (HDFS_DELEGATION_TOKEN token 860 for lngo)\n",
      "16/07/25 15:31:38 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/07/25 15:31:38 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.217:1019\n",
      "16/07/25 15:31:38 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.204:1019\n",
      "16/07/25 15:31:38 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.218:1019\n",
      "16/07/25 15:31:38 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.222:1019\n",
      "16/07/25 15:31:38 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/07/25 15:31:38 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1469462752582_0010\n",
      "16/07/25 15:31:38 INFO mapreduce.JobSubmitter: Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:dsci, Ident: (HDFS_DELEGATION_TOKEN token 860 for lngo)\n",
      "16/07/25 15:31:39 INFO impl.YarnClientImpl: Submitted application application_1469462752582_0010\n",
      "16/07/25 15:31:39 INFO mapreduce.Job: The url to track the job: http://dscim001.palmetto.clemson.edu:8088/proxy/application_1469462752582_0010/\n",
      "16/07/25 15:31:39 INFO mapreduce.Job: Running job: job_1469462752582_0010\n",
      "16/07/25 15:31:54 INFO mapreduce.Job: Job job_1469462752582_0010 running in uber mode : false\n",
      "16/07/25 15:31:54 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/07/25 15:32:13 INFO mapreduce.Job:  map 4% reduce 0%\n",
      "16/07/25 15:32:16 INFO mapreduce.Job:  map 8% reduce 0%\n",
      "16/07/25 15:32:19 INFO mapreduce.Job:  map 12% reduce 0%\n",
      "16/07/25 15:32:22 INFO mapreduce.Job:  map 16% reduce 0%\n",
      "16/07/25 15:32:25 INFO mapreduce.Job:  map 21% reduce 0%\n",
      "16/07/25 15:32:28 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "16/07/25 15:32:31 INFO mapreduce.Job:  map 27% reduce 0%\n",
      "16/07/25 15:32:32 INFO mapreduce.Job:  map 29% reduce 0%\n",
      "16/07/25 15:32:36 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "16/07/25 15:32:39 INFO mapreduce.Job:  map 37% reduce 0%\n",
      "16/07/25 15:32:42 INFO mapreduce.Job:  map 41% reduce 0%\n",
      "16/07/25 15:32:45 INFO mapreduce.Job:  map 45% reduce 0%\n",
      "16/07/25 15:32:48 INFO mapreduce.Job:  map 48% reduce 0%\n",
      "16/07/25 15:32:51 INFO mapreduce.Job:  map 52% reduce 0%\n",
      "16/07/25 15:32:54 INFO mapreduce.Job:  map 54% reduce 0%\n",
      "16/07/25 15:32:55 INFO mapreduce.Job:  map 56% reduce 0%\n",
      "16/07/25 15:32:58 INFO mapreduce.Job:  map 62% reduce 0%\n",
      "16/07/25 15:33:01 INFO mapreduce.Job:  map 66% reduce 0%\n",
      "16/07/25 15:33:04 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/07/25 15:33:13 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "16/07/25 15:33:19 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/07/25 15:33:30 INFO mapreduce.Job:  map 100% reduce 46%\n",
      "16/07/25 15:33:33 INFO mapreduce.Job:  map 100% reduce 56%\n",
      "16/07/25 15:33:36 INFO mapreduce.Job:  map 100% reduce 67%\n",
      "16/07/25 15:33:39 INFO mapreduce.Job:  map 100% reduce 69%\n",
      "16/07/25 15:33:42 INFO mapreduce.Job:  map 100% reduce 71%\n",
      "16/07/25 15:33:45 INFO mapreduce.Job:  map 100% reduce 74%\n",
      "16/07/25 15:33:49 INFO mapreduce.Job:  map 100% reduce 76%\n",
      "16/07/25 15:33:52 INFO mapreduce.Job:  map 100% reduce 78%\n",
      "16/07/25 15:33:55 INFO mapreduce.Job:  map 100% reduce 81%\n",
      "16/07/25 15:33:58 INFO mapreduce.Job:  map 100% reduce 85%\n",
      "16/07/25 15:34:01 INFO mapreduce.Job:  map 100% reduce 88%\n",
      "16/07/25 15:34:04 INFO mapreduce.Job:  map 100% reduce 91%\n",
      "16/07/25 15:34:07 INFO mapreduce.Job:  map 100% reduce 94%\n",
      "16/07/25 15:34:10 INFO mapreduce.Job:  map 100% reduce 98%\n",
      "16/07/25 15:34:12 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/07/25 15:34:13 INFO mapreduce.Job: Job job_1469462752582_0010 completed successfully\n",
      "16/07/25 15:34:13 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=315119692\n",
      "\t\tFILE: Number of bytes written=630684946\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=265236931\n",
      "\t\tHDFS: Number of bytes written=422298\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=158762\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=114720\n",
      "\t\tTotal time spent by all map tasks (ms)=158762\n",
      "\t\tTotal time spent by all reduce tasks (ms)=57360\n",
      "\t\tTotal vcore-seconds taken by all map tasks=158762\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=57360\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=1300578304\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=939786240\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10000054\n",
      "\t\tMap output records=10000054\n",
      "\t\tMap output bytes=295119333\n",
      "\t\tMap output materialized bytes=315119698\n",
      "\t\tInput split bytes=224\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=10676\n",
      "\t\tReduce shuffle bytes=315119698\n",
      "\t\tReduce input records=10000054\n",
      "\t\tReduce output records=10676\n",
      "\t\tSpilled Records=20000108\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=1832\n",
      "\t\tCPU time spent (ms)=252730\n",
      "\t\tPhysical memory (bytes) snapshot=6152097792\n",
      "\t\tVirtual memory (bytes) snapshot=34582532096\n",
      "\t\tTotal committed heap usage (bytes)=7099383808\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=265236707\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=422298\n",
      "16/07/25 15:34:13 INFO streaming.StreamJob: Output directory: ratings\n"
     ]
    }
   ],
   "source": [
    "!ssh dsciu001 yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input /user/lngo/intro-to-hadoop/ml-10M100K/ratings.dat  \\\n",
    "    -output ratings \\\n",
    "    -file /home/lngo/intro-to-hadoop/mapper02.py \\\n",
    "    -mapper mapper02.py \\\n",
    "    -file /home/lngo/intro-to-hadoop/reducer01.py \\\n",
    "    -reducer reducer01.py \\\n",
    "    -file /home/lngo/intro-to-hadoop/movies.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The content of the ratings directory includes an empty file serves as a flag to\n",
    "indicate whether the operation was successful or not, and the output files. The\n",
    "number of output files depends on how many reducers we use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   2 lngo hdfs          0 2016-07-25 15:34 ratings/_SUCCESS\r\n",
      "-rw-r--r--   2 lngo hdfs     422298 2016-07-25 15:34 ratings/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "!ssh dsciu001 hdfs dfs -ls ratings 2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can **cat** for the content of the output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Great Performances\" Cats (1998)\t3.58333333333\r\n",
      "'Round Midnight (1986)\t3.72\r\n",
      "'Til There Was You (1997)\t2.83774834437\r\n",
      "'burbs, The (1989)\t2.96941489362\r\n",
      "'night Mother (1986)\t3.45023696682\r\n",
      "*batteries not included (1987)\t3.15314401623\r\n",
      "...All the Marbles (a.k.a. The California Dolls) (1981)\t2.21739130435\r\n",
      "...And God Created Woman (Et Dieu... créa la femme) (1956)\t3.08552631579\r\n",
      "...And God Spoke (1993)\t3.28260869565\r\n",
      "...And Justice for All (1979)\t3.65270935961\r\n"
     ]
    }
   ],
   "source": [
    "!ssh dsciu001 hdfs dfs -cat ratings/part-00000 2>/dev/null | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to increase number of reducers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/07/25 15:36:57 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/home/lngo/intro-to-hadoop/mapper02.py, /home/lngo/intro-to-hadoop/reducer01.py, /home/lngo/intro-to-hadoop/movies.dat] [/usr/hdp/2.4.2.0-258/hadoop-mapreduce/hadoop-streaming-2.7.1.2.4.2.0-258.jar] /var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir/streamjob4268970964328268399.jar tmpDir=null\n",
      "16/07/25 15:36:58 INFO impl.TimelineClientImpl: Timeline service address: http://dscim003.palmetto.clemson.edu:8188/ws/v1/timeline/\n",
      "16/07/25 15:36:59 INFO impl.TimelineClientImpl: Timeline service address: http://dscim003.palmetto.clemson.edu:8188/ws/v1/timeline/\n",
      "16/07/25 15:36:59 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 861 for lngo on ha-hdfs:dsci\n",
      "16/07/25 15:36:59 INFO security.TokenCache: Got dt for hdfs://dsci; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:dsci, Ident: (HDFS_DELEGATION_TOKEN token 861 for lngo)\n",
      "16/07/25 15:37:00 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/07/25 15:37:00 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.204:1019\n",
      "16/07/25 15:37:00 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.217:1019\n",
      "16/07/25 15:37:00 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.222:1019\n",
      "16/07/25 15:37:00 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.218:1019\n",
      "16/07/25 15:37:00 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/07/25 15:37:00 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1469462752582_0011\n",
      "16/07/25 15:37:00 INFO mapreduce.JobSubmitter: Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:dsci, Ident: (HDFS_DELEGATION_TOKEN token 861 for lngo)\n",
      "16/07/25 15:37:01 INFO impl.YarnClientImpl: Submitted application application_1469462752582_0011\n",
      "16/07/25 15:37:01 INFO mapreduce.Job: The url to track the job: http://dscim001.palmetto.clemson.edu:8088/proxy/application_1469462752582_0011/\n",
      "16/07/25 15:37:01 INFO mapreduce.Job: Running job: job_1469462752582_0011\n",
      "16/07/25 15:37:16 INFO mapreduce.Job: Job job_1469462752582_0011 running in uber mode : false\n",
      "16/07/25 15:37:16 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/07/25 15:37:34 INFO mapreduce.Job:  map 4% reduce 0%\n",
      "16/07/25 15:37:37 INFO mapreduce.Job:  map 9% reduce 0%\n",
      "16/07/25 15:37:40 INFO mapreduce.Job:  map 13% reduce 0%\n",
      "16/07/25 15:37:43 INFO mapreduce.Job:  map 17% reduce 0%\n",
      "16/07/25 15:37:46 INFO mapreduce.Job:  map 21% reduce 0%\n",
      "16/07/25 15:37:49 INFO mapreduce.Job:  map 23% reduce 0%\n",
      "16/07/25 15:37:50 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "16/07/25 15:37:52 INFO mapreduce.Job:  map 26% reduce 0%\n",
      "16/07/25 15:37:53 INFO mapreduce.Job:  map 28% reduce 0%\n",
      "16/07/25 15:37:56 INFO mapreduce.Job:  map 32% reduce 0%\n",
      "16/07/25 15:37:59 INFO mapreduce.Job:  map 36% reduce 0%\n",
      "16/07/25 15:38:02 INFO mapreduce.Job:  map 40% reduce 0%\n",
      "16/07/25 15:38:05 INFO mapreduce.Job:  map 44% reduce 0%\n",
      "16/07/25 15:38:08 INFO mapreduce.Job:  map 48% reduce 0%\n",
      "16/07/25 15:38:11 INFO mapreduce.Job:  map 53% reduce 0%\n",
      "16/07/25 15:38:14 INFO mapreduce.Job:  map 59% reduce 0%\n",
      "16/07/25 15:38:17 INFO mapreduce.Job:  map 62% reduce 0%\n",
      "16/07/25 15:38:18 INFO mapreduce.Job:  map 64% reduce 0%\n",
      "16/07/25 15:38:21 INFO mapreduce.Job:  map 65% reduce 0%\n",
      "16/07/25 15:38:24 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/07/25 15:38:32 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "16/07/25 15:38:39 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/07/25 15:38:50 INFO mapreduce.Job:  map 100% reduce 73%\n",
      "16/07/25 15:38:53 INFO mapreduce.Job:  map 100% reduce 78%\n",
      "16/07/25 15:38:54 INFO mapreduce.Job:  map 100% reduce 84%\n",
      "16/07/25 15:38:55 INFO mapreduce.Job:  map 100% reduce 85%\n",
      "16/07/25 15:38:56 INFO mapreduce.Job:  map 100% reduce 91%\n",
      "16/07/25 15:38:57 INFO mapreduce.Job:  map 100% reduce 93%\n",
      "16/07/25 15:38:59 INFO mapreduce.Job:  map 100% reduce 98%\n",
      "16/07/25 15:39:00 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/07/25 15:39:01 INFO mapreduce.Job: Job job_1469462752582_0011 completed successfully\n",
      "16/07/25 15:39:01 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=315119710\n",
      "\t\tFILE: Number of bytes written=631130596\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=265197906\n",
      "\t\tHDFS: Number of bytes written=422298\n",
      "\t\tHDFS: Number of read operations=18\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=154519\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=188586\n",
      "\t\tTotal time spent by all map tasks (ms)=154519\n",
      "\t\tTotal time spent by all reduce tasks (ms)=94293\n",
      "\t\tTotal vcore-seconds taken by all map tasks=154519\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=94293\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=1265819648\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=1544896512\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10000054\n",
      "\t\tMap output records=10000054\n",
      "\t\tMap output bytes=295119333\n",
      "\t\tMap output materialized bytes=315119734\n",
      "\t\tInput split bytes=224\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=10676\n",
      "\t\tReduce shuffle bytes=315119734\n",
      "\t\tReduce input records=10000054\n",
      "\t\tReduce output records=10676\n",
      "\t\tSpilled Records=20000108\n",
      "\t\tShuffled Maps =8\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=8\n",
      "\t\tGC time elapsed (ms)=2064\n",
      "\t\tCPU time spent (ms)=271690\n",
      "\t\tPhysical memory (bytes) snapshot=7317250048\n",
      "\t\tVirtual memory (bytes) snapshot=83353436160\n",
      "\t\tTotal committed heap usage (bytes)=8577351680\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=265197682\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=422298\n",
      "16/07/25 15:39:01 INFO streaming.StreamJob: Output directory: ratings4R\n"
     ]
    }
   ],
   "source": [
    "!ssh dsciu001 yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -D mapreduce.job.reduces=4 \\\n",
    "    -input /user/lngo/intro-to-hadoop/ml-10M100K/ratings.dat  \\\n",
    "    -output ratings4R \\\n",
    "    -file /home/lngo/intro-to-hadoop/mapper02.py \\\n",
    "    -mapper mapper02.py \\\n",
    "    -file /home/lngo/intro-to-hadoop/reducer01.py \\\n",
    "    -reducer reducer01.py \\\n",
    "    -file /home/lngo/intro-to-hadoop/movies.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\r\n",
      "-rw-r--r--   2 lngo hdfs          0 2016-07-25 15:38 ratings4R/_SUCCESS\r\n",
      "-rw-r--r--   2 lngo hdfs     106498 2016-07-25 15:38 ratings4R/part-00000\r\n",
      "-rw-r--r--   2 lngo hdfs     103491 2016-07-25 15:38 ratings4R/part-00001\r\n",
      "-rw-r--r--   2 lngo hdfs     104521 2016-07-25 15:38 ratings4R/part-00002\r\n",
      "-rw-r--r--   2 lngo hdfs     107788 2016-07-25 15:38 ratings4R/part-00003\r\n"
     ]
    }
   ],
   "source": [
    "!ssh dsciu001 hdfs dfs -ls ratings4R 2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from performance implication, an important difference between using one\n",
    "and many reducers is demonstrated in cases where we want to perform operations\n",
    "that require a global examination of the data. Let's say the movie company\n",
    "wishes to identify the movie with highest rating average.\n",
    "\n",
    "Create a file called **reduce03.py** with the following content\n",
    "\n",
    "~~~ {.output}\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "current_movie = None\n",
    "current_rating_sum = 0\n",
    "current_rating_count = 0\n",
    "\n",
    "max_movie = \"\"\n",
    "max_average = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "  line = line.strip()\n",
    "  movie, rating = line.split(\"\\t\", 1)\n",
    "  try:\n",
    "    rating = float(rating)\n",
    "  except ValueError:\n",
    "    continue\n",
    "\n",
    "  if current_movie == movie:\n",
    "    current_rating_sum += rating\n",
    "    current_rating_count += 1\n",
    "  else:\n",
    "    if current_movie:\n",
    "      rating_average = current_rating_sum / current_rating_count\n",
    "      if rating_average > max_average:\n",
    "        max_movie = current_movie\n",
    "        max_average = rating_average\n",
    "    current_movie = movie\n",
    "    current_rating_sum = rating\n",
    "    current_rating_count = 1\n",
    "\n",
    "if current_movie == movie:\n",
    "  rating_average = current_rating_sum / current_rating_count\n",
    "  if rating_average > max_average:\n",
    "    max_movie = current_movie\n",
    "    max_average = rating_average\n",
    "\n",
    "print (\"%s\\t%s\" % (max_movie, max_average))\n",
    "~~~\n",
    "\n",
    "Rerun the Hadoop program using one and four reducers, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/07/25 15:46:22 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/home/lngo/intro-to-hadoop/mapper02.py, /home/lngo/intro-to-hadoop/reducer03.py, /home/lngo/intro-to-hadoop/movies.dat] [/usr/hdp/2.4.2.0-258/hadoop-mapreduce/hadoop-streaming-2.7.1.2.4.2.0-258.jar] /var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir/streamjob4500088075814474419.jar tmpDir=null\n",
      "16/07/25 15:46:23 INFO impl.TimelineClientImpl: Timeline service address: http://dscim003.palmetto.clemson.edu:8188/ws/v1/timeline/\n",
      "16/07/25 15:46:24 INFO impl.TimelineClientImpl: Timeline service address: http://dscim003.palmetto.clemson.edu:8188/ws/v1/timeline/\n",
      "16/07/25 15:46:24 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 862 for lngo on ha-hdfs:dsci\n",
      "16/07/25 15:46:24 INFO security.TokenCache: Got dt for hdfs://dsci; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:dsci, Ident: (HDFS_DELEGATION_TOKEN token 862 for lngo)\n",
      "16/07/25 15:46:25 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/07/25 15:46:25 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.204:1019\n",
      "16/07/25 15:46:25 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.217:1019\n",
      "16/07/25 15:46:25 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.218:1019\n",
      "16/07/25 15:46:25 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.222:1019\n",
      "16/07/25 15:46:25 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/07/25 15:46:25 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1469462752582_0012\n",
      "16/07/25 15:46:25 INFO mapreduce.JobSubmitter: Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:dsci, Ident: (HDFS_DELEGATION_TOKEN token 862 for lngo)\n",
      "16/07/25 15:46:26 INFO impl.YarnClientImpl: Submitted application application_1469462752582_0012\n",
      "16/07/25 15:46:26 INFO mapreduce.Job: The url to track the job: http://dscim001.palmetto.clemson.edu:8088/proxy/application_1469462752582_0012/\n",
      "16/07/25 15:46:26 INFO mapreduce.Job: Running job: job_1469462752582_0012\n",
      "16/07/25 15:46:40 INFO mapreduce.Job: Job job_1469462752582_0012 running in uber mode : false\n",
      "16/07/25 15:46:40 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/07/25 15:46:57 INFO mapreduce.Job:  map 2% reduce 0%\n",
      "16/07/25 15:46:58 INFO mapreduce.Job:  map 4% reduce 0%\n",
      "16/07/25 15:47:00 INFO mapreduce.Job:  map 6% reduce 0%\n",
      "16/07/25 15:47:01 INFO mapreduce.Job:  map 8% reduce 0%\n",
      "16/07/25 15:47:03 INFO mapreduce.Job:  map 10% reduce 0%\n",
      "16/07/25 15:47:04 INFO mapreduce.Job:  map 12% reduce 0%\n",
      "16/07/25 15:47:07 INFO mapreduce.Job:  map 16% reduce 0%\n",
      "16/07/25 15:47:10 INFO mapreduce.Job:  map 19% reduce 0%\n",
      "16/07/25 15:47:11 INFO mapreduce.Job:  map 21% reduce 0%\n",
      "16/07/25 15:47:13 INFO mapreduce.Job:  map 24% reduce 0%\n",
      "16/07/25 15:47:14 INFO mapreduce.Job:  map 26% reduce 0%\n",
      "16/07/25 15:47:16 INFO mapreduce.Job:  map 29% reduce 0%\n",
      "16/07/25 15:47:17 INFO mapreduce.Job:  map 31% reduce 0%\n",
      "16/07/25 15:47:20 INFO mapreduce.Job:  map 38% reduce 0%\n",
      "16/07/25 15:47:23 INFO mapreduce.Job:  map 44% reduce 0%\n",
      "16/07/25 15:47:26 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/07/25 15:47:29 INFO mapreduce.Job:  map 56% reduce 0%\n",
      "16/07/25 15:47:32 INFO mapreduce.Job:  map 61% reduce 0%\n",
      "16/07/25 15:47:35 INFO mapreduce.Job:  map 66% reduce 0%\n",
      "16/07/25 15:47:40 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/07/25 15:47:52 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/07/25 15:48:09 INFO mapreduce.Job:  map 100% reduce 80%\n",
      "16/07/25 15:48:12 INFO mapreduce.Job:  map 100% reduce 91%\n",
      "16/07/25 15:48:13 INFO mapreduce.Job:  map 100% reduce 94%\n",
      "16/07/25 15:48:14 INFO mapreduce.Job:  map 100% reduce 95%\n",
      "16/07/25 15:48:15 INFO mapreduce.Job:  map 100% reduce 97%\n",
      "16/07/25 15:48:16 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/07/25 15:48:17 INFO mapreduce.Job: Job job_1469462752582_0012 completed successfully\n",
      "16/07/25 15:48:17 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=315119710\n",
      "\t\tFILE: Number of bytes written=631130602\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=265197906\n",
      "\t\tHDFS: Number of bytes written=180\n",
      "\t\tHDFS: Number of read operations=18\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=138245\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=166412\n",
      "\t\tTotal time spent by all map tasks (ms)=138245\n",
      "\t\tTotal time spent by all reduce tasks (ms)=83206\n",
      "\t\tTotal vcore-seconds taken by all map tasks=138245\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=83206\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=1132503040\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=1363247104\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10000054\n",
      "\t\tMap output records=10000054\n",
      "\t\tMap output bytes=295119333\n",
      "\t\tMap output materialized bytes=315119734\n",
      "\t\tInput split bytes=224\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=10676\n",
      "\t\tReduce shuffle bytes=315119734\n",
      "\t\tReduce input records=10000054\n",
      "\t\tReduce output records=4\n",
      "\t\tSpilled Records=20000108\n",
      "\t\tShuffled Maps =8\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=8\n",
      "\t\tGC time elapsed (ms)=2006\n",
      "\t\tCPU time spent (ms)=240550\n",
      "\t\tPhysical memory (bytes) snapshot=7421755392\n",
      "\t\tVirtual memory (bytes) snapshot=83398647808\n",
      "\t\tTotal committed heap usage (bytes)=8729395200\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=265197682\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=180\n",
      "16/07/25 15:48:17 INFO streaming.StreamJob: Output directory: ratingsMax\n"
     ]
    }
   ],
   "source": [
    "!ssh dsciu001 yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -D mapreduce.job.reduces=4 \\\n",
    "    -input /user/lngo/intro-to-hadoop/ml-10M100K/ratings.dat  \\\n",
    "    -output ratingsMax \\\n",
    "    -file /home/lngo/intro-to-hadoop/mapper02.py \\\n",
    "    -mapper mapper02.py \\\n",
    "    -file /home/lngo/intro-to-hadoop/reducer03.py \\\n",
    "    -reducer reducer03.py \\\n",
    "    -file /home/lngo/intro-to-hadoop/movies.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/07/25 15:53:18 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/home/lngo/intro-to-hadoop/mapper02.py, /home/lngo/intro-to-hadoop/reducer03.py, /home/lngo/intro-to-hadoop/movies.dat] [/usr/hdp/2.4.2.0-258/hadoop-mapreduce/hadoop-streaming-2.7.1.2.4.2.0-258.jar] /var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir/streamjob3588979903194305897.jar tmpDir=null\n",
      "16/07/25 15:53:19 INFO impl.TimelineClientImpl: Timeline service address: http://dscim003.palmetto.clemson.edu:8188/ws/v1/timeline/\n",
      "16/07/25 15:53:20 INFO impl.TimelineClientImpl: Timeline service address: http://dscim003.palmetto.clemson.edu:8188/ws/v1/timeline/\n",
      "16/07/25 15:53:20 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 864 for lngo on ha-hdfs:dsci\n",
      "16/07/25 15:53:20 INFO security.TokenCache: Got dt for hdfs://dsci; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:dsci, Ident: (HDFS_DELEGATION_TOKEN token 864 for lngo)\n",
      "16/07/25 15:53:20 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/07/25 15:53:20 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.217:1019\n",
      "16/07/25 15:53:20 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.204:1019\n",
      "16/07/25 15:53:20 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.218:1019\n",
      "16/07/25 15:53:20 INFO net.NetworkTopology: Adding a new node: /default-rack/10.125.8.222:1019\n",
      "16/07/25 15:53:21 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/07/25 15:53:21 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1469462752582_0014\n",
      "16/07/25 15:53:21 INFO mapreduce.JobSubmitter: Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:dsci, Ident: (HDFS_DELEGATION_TOKEN token 864 for lngo)\n",
      "16/07/25 15:53:21 INFO impl.YarnClientImpl: Submitted application application_1469462752582_0014\n",
      "16/07/25 15:53:22 INFO mapreduce.Job: The url to track the job: http://dscim001.palmetto.clemson.edu:8088/proxy/application_1469462752582_0014/\n",
      "16/07/25 15:53:22 INFO mapreduce.Job: Running job: job_1469462752582_0014\n",
      "16/07/25 15:53:36 INFO mapreduce.Job: Job job_1469462752582_0014 running in uber mode : false\n",
      "16/07/25 15:53:36 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/07/25 15:53:53 INFO mapreduce.Job:  map 4% reduce 0%\n",
      "16/07/25 15:53:56 INFO mapreduce.Job:  map 8% reduce 0%\n",
      "16/07/25 15:53:59 INFO mapreduce.Job:  map 12% reduce 0%\n",
      "16/07/25 15:54:02 INFO mapreduce.Job:  map 16% reduce 0%\n",
      "16/07/25 15:54:05 INFO mapreduce.Job:  map 21% reduce 0%\n",
      "16/07/25 15:54:08 INFO mapreduce.Job:  map 23% reduce 0%\n",
      "16/07/25 15:54:09 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "16/07/25 15:54:12 INFO mapreduce.Job:  map 29% reduce 0%\n",
      "16/07/25 15:54:15 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "16/07/25 15:54:18 INFO mapreduce.Job:  map 38% reduce 0%\n",
      "16/07/25 15:54:21 INFO mapreduce.Job:  map 42% reduce 0%\n",
      "16/07/25 15:54:24 INFO mapreduce.Job:  map 46% reduce 0%\n",
      "16/07/25 15:54:27 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/07/25 15:54:30 INFO mapreduce.Job:  map 55% reduce 0%\n",
      "16/07/25 15:54:34 INFO mapreduce.Job:  map 60% reduce 0%\n",
      "16/07/25 15:54:37 INFO mapreduce.Job:  map 65% reduce 0%\n",
      "16/07/25 15:54:40 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/07/25 15:54:52 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "16/07/25 15:54:57 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/07/25 15:55:09 INFO mapreduce.Job:  map 100% reduce 38%\n",
      "16/07/25 15:55:10 INFO mapreduce.Job:  map 100% reduce 78%\n",
      "16/07/25 15:55:13 INFO mapreduce.Job:  map 100% reduce 89%\n",
      "16/07/25 15:55:16 INFO mapreduce.Job:  map 100% reduce 98%\n",
      "16/07/25 15:55:18 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/07/25 15:55:19 INFO mapreduce.Job: Job job_1469462752582_0014 completed successfully\n",
      "16/07/25 15:55:19 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=315119710\n",
      "\t\tFILE: Number of bytes written=631130614\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=265197906\n",
      "\t\tHDFS: Number of bytes written=180\n",
      "\t\tHDFS: Number of read operations=18\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=154378\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=174446\n",
      "\t\tTotal time spent by all map tasks (ms)=154378\n",
      "\t\tTotal time spent by all reduce tasks (ms)=87223\n",
      "\t\tTotal vcore-seconds taken by all map tasks=154378\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=87223\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=1264664576\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=1429061632\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10000054\n",
      "\t\tMap output records=10000054\n",
      "\t\tMap output bytes=295119333\n",
      "\t\tMap output materialized bytes=315119734\n",
      "\t\tInput split bytes=224\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=10676\n",
      "\t\tReduce shuffle bytes=315119734\n",
      "\t\tReduce input records=10000054\n",
      "\t\tReduce output records=4\n",
      "\t\tSpilled Records=20000108\n",
      "\t\tShuffled Maps =8\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=8\n",
      "\t\tGC time elapsed (ms)=2074\n",
      "\t\tCPU time spent (ms)=271730\n",
      "\t\tPhysical memory (bytes) snapshot=7329378304\n",
      "\t\tVirtual memory (bytes) snapshot=83325136896\n",
      "\t\tTotal committed heap usage (bytes)=8554283008\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=265197682\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=180\n",
      "16/07/25 15:55:19 INFO streaming.StreamJob: Output directory: ratingsMax4R\n"
     ]
    }
   ],
   "source": [
    "!ssh dsciu001 yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -D mapreduce.job.reduces=4 \\\n",
    "    -input /user/lngo/intro-to-hadoop/ml-10M100K/ratings.dat  \\\n",
    "    -output ratingsMax4R \\\n",
    "    -file /home/lngo/intro-to-hadoop/mapper02.py \\\n",
    "    -mapper mapper02.py \\\n",
    "    -file /home/lngo/intro-to-hadoop/reducer03.py \\\n",
    "    -reducer reducer03.py \\\n",
    "    -file /home/lngo/intro-to-hadoop/movies.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of one reducer, there is only a single answer for the movie with\n",
    "highest rating average. With four reducers, we have four possible answers. On\n",
    "the other hand, it is quite feasible to infer the final single answer from a\n",
    "set of four possible choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blue Light, The (Das Blaue Licht) (1932)\t5.0\r\n"
     ]
    }
   ],
   "source": [
    "!ssh dsciu001 hdfs dfs -cat ratingsMax/part-00000 2>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blue Light, The (Das Blaue Licht) (1932)\t5.0\r\n"
     ]
    }
   ],
   "source": [
    "!ssh dsciu001 hdfs dfs -cat ratingsMax4R/part-00000 2>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Satan's Tango (Sátántangó) (1994)\t5.0\r\n"
     ]
    }
   ],
   "source": [
    "!ssh dsciu001 hdfs dfs -cat ratingsMax4R/part-00001 2>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Summer, The (Kohayagawa-ke no aki) (1961)\t4.5\r\n"
     ]
    }
   ],
   "source": [
    "!ssh dsciu001 hdfs dfs -cat ratingsMax4R/part-00002 2>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fighting Elegy (Kenka erejii) (1966)\t5.0\r\n"
     ]
    }
   ],
   "source": [
    "!ssh dsciu001 hdfs dfs -cat ratingsMax4R/part-00003 2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Check your understanding: Additional conditions on the reduce side {.challenge}\n",
    "The previous results do not make sense intuitively, as these movies are not\n",
    "well known. It is possible that our results are skewed by movies having too\n",
    "few reviews. Modify the reducer so that we only consider movies that have more\n",
    "than one thousand ratings totally. Name this reducer reducer03.py. Run the\n",
    "Hadoop MapReduce program again with mapper03.py and reducer03.py using one and\n",
    "four reducers respectively. Report the outcome.\n",
    "\n",
    "\n",
    "\n",
    "## Check your understanding: User Study {.challenge}\n",
    "User feedback plays an important role in marketing strategies. Implement a\n",
    "Hadoop MapReduce program that identifies the user that rates the most movies\n",
    "over time. Identify the genre that this user rates most favorably."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda 2.5.0 (Python 3)",
   "language": "python",
   "name": "anaconda_2.5.0_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
