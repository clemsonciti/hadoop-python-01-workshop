{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "---\n",
    "layout: page\n",
    "title: Introduction to Hadoop\n",
    "subtitle: Integrating Python Mapper and Reducer in Hadoop\n",
    "minutes: 15\n",
    "---\n",
    "> ## Learning Objectives {.objectives}\n",
    ">\n",
    "> *   Run the combination of Python-based mapper and reducer on the Hadoop\n",
    ">     infrastructure\n",
    "> *   Customize reducer for questions that require global access to KEYS\n",
    "\n",
    "With the mapper and reducer created and tested, the final step is to run this\n",
    "combination on the Hadoop infrastructure.\n",
    "\n",
    "~~~ {.bash}\n",
    "!ssh dsciu001 yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar -input /repository/movielens/ratings.csv  -output ratings -file /home/<username>/mapreduce/mapper03.py -mapper mapper03.py -file /home/<username>/mapreduce/mapper03.py -reducer reducer01.py -file /home/<username>/mapreduce/movies.csv\n",
    "~~~\n",
    "\n",
    "~~~ {.output}\n",
    "16/07/06 12:30:13 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
    "packageJobJar: [/home/lngo/mapreduce/mapper03.py, /home/lngo/mapreduce/reducer01.py, /home/lngo/mapreduce/movies.csv] [/usr/hdp/2.4.2.0-258/hadoop-mapreduce/hadoop-streaming-2.7.1.2.4.2.0-258.jar] /var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir/streamjob1141304963588025269.jar tmpDir=null\n",
    "16/07/06 12:30:15 INFO impl.TimelineClientImpl: Timeline service address: http://dscim003.palmetto.clemson.edu:8188/ws/v1/timeline/\n",
    "16/07/06 12:30:15 INFO impl.TimelineClientImpl: Timeline service address: http://dscim003.palmetto.clemson.edu:8188/ws/v1/timeline/\n",
    "16/07/06 12:30:16 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
    "16/07/06 12:30:16 INFO mapreduce.JobSubmitter: number of splits:5\n",
    "16/07/06 12:30:16 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1467819539655_0009\n",
    "16/07/06 12:30:16 INFO impl.YarnClientImpl: Submitted application application_1467819539655_0009\n",
    "16/07/06 12:30:17 INFO mapreduce.Job: The url to track the job: http://dscim001.palmetto.clemson.edu:8088/proxy/application_1467819539655_0009/\n",
    "16/07/06 12:30:17 INFO mapreduce.Job: Running job: job_1467819539655_0009\n",
    "16/07/06 12:30:26 INFO mapreduce.Job: Job job_1467819539655_0009 running in uber mode : false\n",
    "16/07/06 12:30:26 INFO mapreduce.Job:  map 0% reduce 0%\n",
    "16/07/06 12:30:39 INFO mapreduce.Job:  map 3% reduce 0%\n",
    "16/07/06 12:30:40 INFO mapreduce.Job:  map 5% reduce 0%\n",
    "16/07/06 12:30:42 INFO mapreduce.Job:  map 8% reduce 0%\n",
    "16/07/06 12:30:43 INFO mapreduce.Job:  map 10% reduce 0%\n",
    "16/07/06 12:30:45 INFO mapreduce.Job:  map 11% reduce 0%\n",
    "16/07/06 12:30:46 INFO mapreduce.Job:  map 14% reduce 0%\n",
    "16/07/06 12:30:48 INFO mapreduce.Job:  map 15% reduce 0%\n",
    "16/07/06 12:30:49 INFO mapreduce.Job:  map 20% reduce 0%\n",
    "16/07/06 12:30:51 INFO mapreduce.Job:  map 21% reduce 0%\n",
    "16/07/06 12:30:52 INFO mapreduce.Job:  map 25% reduce 0%\n",
    "16/07/06 12:30:55 INFO mapreduce.Job:  map 30% reduce 0%\n",
    "16/07/06 12:30:58 INFO mapreduce.Job:  map 35% reduce 0%\n",
    "16/07/06 12:31:01 INFO mapreduce.Job:  map 39% reduce 0%\n",
    "16/07/06 12:31:02 INFO mapreduce.Job:  map 40% reduce 0%\n",
    "16/07/06 12:31:04 INFO mapreduce.Job:  map 44% reduce 0%\n",
    "16/07/06 12:31:05 INFO mapreduce.Job:  map 46% reduce 0%\n",
    "16/07/06 12:31:07 INFO mapreduce.Job:  map 48% reduce 0%\n",
    "16/07/06 12:31:08 INFO mapreduce.Job:  map 50% reduce 0%\n",
    "16/07/06 12:31:10 INFO mapreduce.Job:  map 51% reduce 0%\n",
    "16/07/06 12:31:11 INFO mapreduce.Job:  map 54% reduce 0%\n",
    "16/07/06 12:31:13 INFO mapreduce.Job:  map 55% reduce 0%\n",
    "16/07/06 12:31:14 INFO mapreduce.Job:  map 58% reduce 0%\n",
    "16/07/06 12:31:16 INFO mapreduce.Job:  map 65% reduce 0%\n",
    "16/07/06 12:31:17 INFO mapreduce.Job:  map 67% reduce 0%\n",
    "16/07/06 12:31:20 INFO mapreduce.Job:  map 70% reduce 0%\n",
    "16/07/06 12:31:23 INFO mapreduce.Job:  map 72% reduce 0%\n",
    "16/07/06 12:31:26 INFO mapreduce.Job:  map 73% reduce 0%\n",
    "16/07/06 12:31:29 INFO mapreduce.Job:  map 73% reduce 7%\n",
    "16/07/06 12:31:32 INFO mapreduce.Job:  map 80% reduce 7%\n",
    "16/07/06 12:31:35 INFO mapreduce.Job:  map 87% reduce 13%\n",
    "16/07/06 12:31:38 INFO mapreduce.Job:  map 87% reduce 20%\n",
    "16/07/06 12:31:39 INFO mapreduce.Job:  map 100% reduce 20%\n",
    "16/07/06 12:31:41 INFO mapreduce.Job:  map 100% reduce 35%\n",
    "16/07/06 12:31:44 INFO mapreduce.Job:  map 100% reduce 41%\n",
    "16/07/06 12:31:48 INFO mapreduce.Job:  map 100% reduce 45%\n",
    "16/07/06 12:31:51 INFO mapreduce.Job:  map 100% reduce 49%\n",
    "16/07/06 12:31:54 INFO mapreduce.Job:  map 100% reduce 54%\n",
    "16/07/06 12:31:57 INFO mapreduce.Job:  map 100% reduce 58%\n",
    "16/07/06 12:32:00 INFO mapreduce.Job:  map 100% reduce 62%\n",
    "16/07/06 12:32:03 INFO mapreduce.Job:  map 100% reduce 67%\n",
    "16/07/06 12:32:06 INFO mapreduce.Job:  map 100% reduce 68%\n",
    "16/07/06 12:32:10 INFO mapreduce.Job:  map 100% reduce 70%\n",
    "16/07/06 12:32:13 INFO mapreduce.Job:  map 100% reduce 71%\n",
    "16/07/06 12:32:16 INFO mapreduce.Job:  map 100% reduce 73%\n",
    "16/07/06 12:32:19 INFO mapreduce.Job:  map 100% reduce 74%\n",
    "16/07/06 12:32:22 INFO mapreduce.Job:  map 100% reduce 76%\n",
    "16/07/06 12:32:25 INFO mapreduce.Job:  map 100% reduce 77%\n",
    "16/07/06 12:32:28 INFO mapreduce.Job:  map 100% reduce 79%\n",
    "16/07/06 12:32:31 INFO mapreduce.Job:  map 100% reduce 80%\n",
    "16/07/06 12:32:35 INFO mapreduce.Job:  map 100% reduce 82%\n",
    "16/07/06 12:32:38 INFO mapreduce.Job:  map 100% reduce 84%\n",
    "16/07/06 12:32:42 INFO mapreduce.Job:  map 100% reduce 85%\n",
    "16/07/06 12:32:45 INFO mapreduce.Job:  map 100% reduce 87%\n",
    "16/07/06 12:32:48 INFO mapreduce.Job:  map 100% reduce 89%\n",
    "16/07/06 12:32:51 INFO mapreduce.Job:  map 100% reduce 90%\n",
    "16/07/06 12:32:54 INFO mapreduce.Job:  map 100% reduce 92%\n",
    "16/07/06 12:32:57 INFO mapreduce.Job:  map 100% reduce 93%\n",
    "16/07/06 12:33:00 INFO mapreduce.Job:  map 100% reduce 95%\n",
    "16/07/06 12:33:03 INFO mapreduce.Job:  map 100% reduce 96%\n",
    "16/07/06 12:33:06 INFO mapreduce.Job:  map 100% reduce 98%\n",
    "16/07/06 12:33:09 INFO mapreduce.Job:  map 100% reduce 99%\n",
    "16/07/06 12:33:13 INFO mapreduce.Job:  map 100% reduce 100%\n",
    "16/07/06 12:33:14 INFO mapreduce.Job: Job job_1467819539655_0009 completed successfully\n",
    "16/07/06 12:33:14 INFO mapreduce.Job: Counters: 50\n",
    "\tFile System Counters\n",
    "\t\tFILE: Number of bytes read=729355490\n",
    "\t\tFILE: Number of bytes written=1459573425\n",
    "\t\tFILE: Number of read operations=0\n",
    "\t\tFILE: Number of large read operations=0\n",
    "\t\tFILE: Number of write operations=0\n",
    "\t\tHDFS: Number of bytes read=620729398\n",
    "\t\tHDFS: Number of bytes written=1546431\n",
    "\t\tHDFS: Number of read operations=18\n",
    "\t\tHDFS: Number of large read operations=0\n",
    "\t\tHDFS: Number of write operations=2\n",
    "\tJob Counters\n",
    "\t\tLaunched map tasks=5\n",
    "\t\tLaunched reduce tasks=1\n",
    "\t\tData-local map tasks=1\n",
    "\t\tRack-local map tasks=4\n",
    "\t\tTotal time spent by all maps in occupied slots (ms)=323066\n",
    "\t\tTotal time spent by all reduces in occupied slots (ms)=228592\n",
    "\t\tTotal time spent by all map tasks (ms)=323066\n",
    "\t\tTotal time spent by all reduce tasks (ms)=114296\n",
    "\t\tTotal vcore-seconds taken by all map tasks=323066\n",
    "\t\tTotal vcore-seconds taken by all reduce tasks=114296\n",
    "\t\tTotal megabyte-seconds taken by all map tasks=2646556672\n",
    "\t\tTotal megabyte-seconds taken by all reduce tasks=1872625664\n",
    "\tMap-Reduce Framework\n",
    "\t\tMap input records=22884378\n",
    "\t\tMap output records=22884377\n",
    "\t\tMap output bytes=683586345\n",
    "\t\tMap output materialized bytes=729355514\n",
    "\t\tInput split bytes=480\n",
    "\t\tCombine input records=0\n",
    "\t\tCombine output records=0\n",
    "\t\tReduce input groups=33647\n",
    "\t\tReduce shuffle bytes=729355514\n",
    "\t\tReduce input records=22884377\n",
    "\t\tReduce output records=33647\n",
    "\t\tSpilled Records=45768754\n",
    "\t\tShuffled Maps =5\n",
    "\t\tFailed Shuffles=0\n",
    "\t\tMerged Map outputs=5\n",
    "\t\tGC time elapsed (ms)=3993\n",
    "\t\tCPU time spent (ms)=553480\n",
    "\t\tPhysical memory (bytes) snapshot=14627205120\n",
    "\t\tVirtual memory (bytes) snapshot=61926924288\n",
    "\t\tTotal committed heap usage (bytes)=16984834048\n",
    "\tShuffle Errors\n",
    "\t\tBAD_ID=0\n",
    "\t\tCONNECTION=0\n",
    "\t\tIO_ERROR=0\n",
    "\t\tWRONG_LENGTH=0\n",
    "\t\tWRONG_MAP=0\n",
    "\t\tWRONG_REDUCE=0\n",
    "\tFile Input Format Counters\n",
    "\t\tBytes Read=620728918\n",
    "\tFile Output Format Counters\n",
    "\t\tBytes Written=1546431\n",
    "16/07/06 12:33:14 INFO streaming.StreamJob: Output directory: ratings\n",
    "~~~\n",
    "\n",
    "The content of the ratings directory includes an empty file serves as a flag to\n",
    "indicate whether the operation was successful or not, and the output files. The\n",
    "number of output files depends on how many reducers we use.\n",
    "\n",
    "~~~ {.bash}\n",
    "!ssh dsciu001 hdfs dfs -ls ratings 2>/dev/null\n",
    "~~~\n",
    "\n",
    "~~~ {.output}\n",
    "Found 2 items\n",
    "-rw-r--r--   2 lngo hdfs          0 2016-07-06 12:33 ratings/_SUCCESS\n",
    "-rw-r--r--   2 lngo hdfs    1546431 2016-07-06 12:33 ratings/part-00000\n",
    "~~~\n",
    "\n",
    "We can **cat** for the content of the output file\n",
    "\n",
    "~~~ {.bash}\n",
    "!ssh dsciu001 hdfs dfs -cat ratings/part-00000 2>/dev/null | head\n",
    "~~~\n",
    "\n",
    "~~~ {.output}\n",
    "\"Great Performances\" Cats (1998)\t2.77536231884\t574.5\t207\n",
    "#1 Cheerleader Camp (2010)\t2.5\t12.5\t5\n",
    "#chicagoGirl: The Social Network Takes on a Dictator (2013)\t3.66666666667\t11.0\t3\n",
    "$ (Dollars) (1971)\t2.74074074074\t74.0\t27\n",
    "$5 a Day (2008)\t2.98\t149.0\t50\n",
    "$9.99 (2008)\t3.15873015873\t199.0\t63\n",
    "$ellebrity (Sellebrity) (2012)\t2.25\t13.5\t6\n",
    "'49-'17 (1917)\t2.5\t2.5\t1\n",
    "'71 (2014)\t3.64705882353\t496.0\t136\n",
    "'Hellboy': The Seeds of Creation (2004)\t3.08878504673\t330.5\t107\n",
    "~~~\n",
    "\n",
    "It is also possible to increase number of reducers\n",
    "\n",
    "~~~ {.bash}\n",
    "!ssh dsciu001 yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar -D mapreduce.job.reduces=4 -input /repository/movielens/ratings.csv  -output ratings3 -file /home/lngo/mapreduce/mapper03.py -mapper /home/lngo/mapreduce/mapper03.py -file /home/lngo/mapreduce/reducer01.py -reducer /home/lngo/mapreduce/reducer01.py -file /home/lngo/mapreduce/movies.csv\n",
    "~~~\n",
    "\n",
    "~~~ {.bash}\n",
    "!ssh dsciu001 hdfs dfs -ls ratings3 2>/dev/null\n",
    "~~~\n",
    "\n",
    "~~~ {.output}\n",
    "Found 5 items\n",
    "-rw-r--r--   2 lngo hdfs          0 2016-07-06 23:22 ratings3/_SUCCESS\n",
    "-rw-r--r--   2 lngo hdfs     392511 2016-07-06 23:22 ratings3/part-00000\n",
    "-rw-r--r--   2 lngo hdfs     384436 2016-07-06 23:21 ratings3/part-00001\n",
    "-rw-r--r--   2 lngo hdfs     383097 2016-07-06 23:22 ratings3/part-00002\n",
    "-rw-r--r--   2 lngo hdfs     386387 2016-07-06 23:21 ratings3/part-00003\n",
    "~~~\n",
    "\n",
    "Aside from performance implication, an important difference between using one\n",
    "and many reducers is demonstrated in cases where we want to perform operations\n",
    "that require a global examination of the data. Let's say the movie company\n",
    "wishes to identify the movie with highest rating average.\n",
    "\n",
    "Create a file called reduce02.py with the following content\n",
    "\n",
    "~~~ {.output}\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "current_movie = None\n",
    "current_rating_sum = 0\n",
    "current_rating_count = 0\n",
    "\n",
    "max_movie = \"\"\n",
    "max_average = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "  line = line.strip()\n",
    "  movie, rating = line.split(\"\\t\", 1)\n",
    "  try:\n",
    "    rating = float(rating)\n",
    "  except ValueError:\n",
    "    continue\n",
    "\n",
    "  if current_movie == movie:\n",
    "    current_rating_sum += rating\n",
    "    current_rating_count += 1\n",
    "  else:\n",
    "    if current_movie:\n",
    "      rating_average = current_rating_sum / current_rating_count\n",
    "      if rating_average > max_average:\n",
    "        max_movie = current_movie\n",
    "        max_average = rating_average\n",
    "    current_movie = movie\n",
    "    current_rating_sum = rating\n",
    "    current_rating_count = 1\n",
    "\n",
    "if current_movie == movie:\n",
    "  rating_average = current_rating_sum / current_rating_count\n",
    "  if rating_average > max_average:\n",
    "    max_movie = current_movie\n",
    "    max_average = rating_average\n",
    "\n",
    "print (\"%s\\t%s\" % (max_movie, max_average))\n",
    "~~~\n",
    "\n",
    "Rerun the Hadoop program using one and four reducers, respectively:\n",
    "\n",
    "~~~ {.bash}\n",
    "!ssh dsciu001 yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar -input /repository/movielens/ratings.csv  -output maxRating -file /home/lngo/mapreduce/mapper03.py -mapper /home/lngo/mapreduce/mapper03.py -file /home/lngo/mapreduce/reducer02.py -reducer /home/lngo/mapreduce/reducer02.py -file /home/lngo/mapreduce/movies.csv\n",
    "~~~\n",
    "\n",
    "~~~ {.bash}\n",
    "!ssh dsciu001 yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar -D mapreduce.job.reduces=4 -input /repository/movielens/ratings.csv  -output maxRating4R -file /home/lngo/mapreduce/mapper03.py -mapper /home/lngo/mapreduce/mapper03.py -file /home/lngo/mapreduce/reducer02.py -reducer /home/lngo/mapreduce/reducer02.py -file /home/lngo/mapreduce/movies.csv\n",
    "~~~\n",
    "\n",
    "In the case of one reducer, there is only a single answer for the movie with\n",
    "highest rating average. With four reducers, we have four possible answers. On\n",
    "the other hand, it is quite feasible to infer the final single answer from a\n",
    "set of four possible choices.\n",
    "\n",
    "~~~ {.bash}\n",
    "!ssh dsciu001 hdfs dfs -cat maxRating/part-00000 2>/dev/null\n",
    "~~~\n",
    "\n",
    "~~~ {.output}\n",
    "A Job to Kill For (2006)\t5.0\n",
    "~~~\n",
    "\n",
    "~~~ {.bash}\n",
    "!ssh dsciu001 hdfs dfs -cat maxRating4R/part-00000 2>/dev/null\n",
    "~~~\n",
    "\n",
    "~~~ {.output}\n",
    "A Job to Kill For (2006)\t5.0\n",
    "~~~\n",
    "\n",
    "~~~ {.bash}\n",
    "!ssh dsciu001 hdfs dfs -cat maxRating4R/part-00001 2>/dev/null\n",
    "~~~\n",
    "\n",
    "~~~ {.output}\n",
    "10 Attitudes (2001)\t5.0\n",
    "~~~\n",
    "\n",
    "~~~ {.bash}\n",
    "!ssh dsciu001 hdfs dfs -cat maxRating4R/part-00002 2>/dev/null\n",
    "~~~\n",
    "\n",
    "~~~ {.output}\n",
    "A Gentle Spirit (1987)\t5.0\n",
    "~~~\n",
    "\n",
    "~~~ {.bash}\n",
    "!ssh dsciu001 hdfs dfs -cat maxRating4R/part-00003 2>/dev/null\n",
    "~~~\n",
    "\n",
    "~~~ {.output}\n",
    "2 (2007)\t5.0\n",
    "~~~\n",
    "\n",
    "\n",
    "## Check your understanding: Additional conditions on the reduce side {.challenge}\n",
    "The previous results do not make sense intuitively, as these movies are not\n",
    "well known. It is possible that our results are skewed by movies having too\n",
    "few reviews. Modify the reducer so that we only consider movies that have more\n",
    "than one thousand ratings totally. Name this reducer reducer03.py. Run the\n",
    "Hadoop MapReduce program again with mapper03.py and reducer03.py using one and\n",
    "four reducers respectively. Report the outcome.\n",
    "\n",
    "\n",
    "\n",
    "## Check your understanding: User Study {.challenge}\n",
    "User feedback plays an important role in marketing strategies. Implement a\n",
    "Hadoop MapReduce program that identifies the user that rates the most movies\n",
    "over time. Identify the genre that this user rates most favorably.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda 2.5.0 (Python 3)",
   "language": "python",
   "name": "anaconda_2.5.0_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
